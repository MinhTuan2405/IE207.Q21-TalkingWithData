# Dagster Orchestration ‚Äî Qu·∫£n l√Ω Lu·ªìng Data & Tracking Flow

> **D·ª± √°n:** TalkingWithData  
> **Engine:** Dagster 1.12+  
> **C·∫≠p nh·∫≠t:** 01/03/2026

---

## M·ª•c l·ª•c

- [Ph·∫ßn 1: Dagster ‚Äî T·ªïng quan](#ph·∫ßn-1-dagster--t·ªïng-quan)
- [Ph·∫ßn 2: C√°c kh√°i ni·ªám c·ªët l√µi](#ph·∫ßn-2-c√°c-kh√°i-ni·ªám-c·ªët-l√µi)
- [Ph·∫ßn 3: Ki·∫øn tr√∫c Dagster trong TalkingWithData](#ph·∫ßn-3-ki·∫øn-tr√∫c-dagster-trong-talkingwithdata)
- [Ph·∫ßn 4: Tri·ªÉn khai Data Pipelines](#ph·∫ßn-4-tri·ªÉn-khai-data-pipelines)
- [Ph·∫ßn 5: Sensors & Schedules ‚Äî Automation](#ph·∫ßn-5-sensors--schedules--automation)
- [Ph·∫ßn 6: Tracking & Observability](#ph·∫ßn-6-tracking--observability)
- [Ph·∫ßn 7: C·∫•u h√¨nh & Deployment](#ph·∫ßn-7-c·∫•u-h√¨nh--deployment)
- [Ph·∫ßn 8: Best Practices & Troubleshooting](#ph·∫ßn-8-best-practices--troubleshooting)

---

# Ph·∫ßn 1: Dagster ‚Äî T·ªïng quan

## 1.1. Dagster l√† g√¨?

**Dagster** l√† m·ªôt **data orchestrator** ‚Äî ph·∫ßn m·ªÅm qu·∫£n l√Ω, l√™n l·ªãch, v√† gi√°m s√°t c√°c pipeline x·ª≠ l√Ω d·ªØ li·ªáu. Kh√°c v·ªõi Airflow (task-centric), Dagster l√† **asset-centric**: focus v√†o **d·ªØ li·ªáu ƒë∆∞·ª£c t·∫°o ra** thay v√¨ **task c·∫ßn ch·∫°y**.

```
Airflow:  Task A ‚Üí Task B ‚Üí Task C     (quan t√¢m "c·∫ßn l√†m g√¨?")
Dagster:  Asset X ‚Üí Asset Y ‚Üí Asset Z  (quan t√¢m "c·∫ßn t·∫°o data g√¨?")
```

## 1.2. T·∫°i sao d√πng Dagster trong TalkingWithData?

TalkingWithData c√≥ nhi·ªÅu lu·ªìng data c·∫ßn ƒë∆∞·ª£c **qu·∫£n l√Ω t·ª± ƒë·ªông** v√† **tracking**:

| Lu·ªìng Data | M√¥ t·∫£ | T·∫ßn su·∫•t |
|-----------|--------|---------|
| **Schema Import** | K·∫øt n·ªëi DB ngu·ªìn ‚Üí extract DDL ‚Üí l∆∞u metadata | Khi user th√™m DB m·ªõi |
| **Schema Embedding** | DDL text ‚Üí Ollama embedding ‚Üí l∆∞u Qdrant | Sau khi import schema |
| **Training Pipeline** | DDL + Docs + Q&A ‚Üí Vanna/LangChain training | Khi c√≥ d·ªØ li·ªáu m·ªõi |
| **Schema Sync** | Ph√°t hi·ªán thay ƒë·ªïi schema ·ªü DB ngu·ªìn ‚Üí c·∫≠p nh·∫≠t | ƒê·ªãnh k·ª≥ (h√†ng gi·ªù/ng√†y) |
| **Analytics** | Th·ªëng k√™ queries, accuracy, usage | Cu·ªëi ng√†y |

**Dagster gi·∫£i quy·∫øt:**
- ‚úÖ **Orchestrate** ‚Äî ƒêi·ªÅu ph·ªëi th·ª© t·ª± c√°c b∆∞·ªõc ƒë√∫ng dependency
- ‚úÖ **Track** ‚Äî Ghi log, visualize m·ªçi pipeline run
- ‚úÖ **Schedule** ‚Äî T·ª± ƒë·ªông ch·∫°y theo l·ªãch ho·∫∑c s·ª± ki·ªán
- ‚úÖ **Retry** ‚Äî T·ª± ch·∫°y l·∫°i khi l·ªói
- ‚úÖ **Observe** ‚Äî Gi√°m s√°t qua UI (localhost:3000)

## 1.3. Ki·∫øn tr√∫c t·ªïng th·ªÉ

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Dagster System                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Webserver   ‚îÇ  ‚îÇ   Daemon     ‚îÇ  ‚îÇ  Code Server   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (UI + API)  ‚îÇ  ‚îÇ  (scheduler, ‚îÇ  ‚îÇ  (gRPC, ch·ª©a   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Port: 3000  ‚îÇ  ‚îÇ   sensors,   ‚îÇ  ‚îÇ   code pipeline‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ   run queue) ‚îÇ  ‚îÇ   Port: 4000)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                   ‚îÇ                                      ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ         ‚îÇ   PostgreSQL      ‚îÇ                            ‚îÇ
‚îÇ         ‚îÇ   (run storage,   ‚îÇ                            ‚îÇ
‚îÇ         ‚îÇ    event log,     ‚îÇ                            ‚îÇ
‚îÇ         ‚îÇ    schedule state)‚îÇ                            ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

         ‚îÇ                     ‚îÇ                   ‚îÇ
         ‚ñº                     ‚ñº                   ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   Ollama    ‚îÇ    ‚îÇ   Qdrant     ‚îÇ    ‚îÇ   Source DB  ‚îÇ
  ‚îÇ   (LLM +   ‚îÇ    ‚îÇ   (Vector    ‚îÇ    ‚îÇ  (PostgreSQL ‚îÇ
  ‚îÇ  embedding) ‚îÇ    ‚îÇ    Store)    ‚îÇ    ‚îÇ   c·ªßa user)  ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# Ph·∫ßn 2: C√°c kh√°i ni·ªám c·ªët l√µi

## 2.1. Assets ‚Äî ƒê∆°n v·ªã d·ªØ li·ªáu

**Asset** = m·ªôt ph·∫ßn d·ªØ li·ªáu ƒë∆∞·ª£c pipeline t·∫°o ra ho·∫∑c c·∫≠p nh·∫≠t. Thay v√¨ nghƒ© "c·∫ßn ch·∫°y task g√¨", ta nghƒ© "c·∫ßn t·∫°o/c·∫≠p nh·∫≠t data g√¨".

```python
from dagster import asset

@asset
def raw_schema_metadata():
    """Asset: metadata schema t·ª´ database ngu·ªìn"""
    # Extract DDL t·ª´ information_schema
    schemas = extract_schemas_from_source_db()
    return schemas

@asset(deps=[raw_schema_metadata])
def schema_embeddings(raw_schema_metadata):
    """Asset: embeddings c·ªßa schema (ph·ª• thu·ªôc raw_schema_metadata)"""
    embeddings = generate_embeddings(raw_schema_metadata)
    store_in_qdrant(embeddings)
    return embeddings
```

**Dependency graph t·ª± ƒë·ªông:**
```
raw_schema_metadata ‚Üí schema_embeddings ‚Üí trained_vanna_model
                   ‚Üò schema_documentation ‚Üó
```

## 2.2. Ops & Jobs ‚Äî ƒê∆°n v·ªã th·ª±c thi

**Op** = m·ªôt function th·ª±c thi (compute unit), **Job** = t·ªï h·ª£p c√°c ops.

```python
from dagster import op, job, In, Out

@op(out=Out(list))
def extract_tables(context):
    """Op: L·∫•y danh s√°ch b·∫£ng t·ª´ DB ngu·ªìn"""
    context.log.info("Extracting tables...")
    tables = get_tables_from_source()
    return tables

@op(ins={"tables": In(list)}, out=Out(dict))
def generate_ddl(context, tables):
    """Op: Sinh DDL cho t·ª´ng b·∫£ng"""
    context.log.info(f"Generating DDL for {len(tables)} tables")
    ddl_map = {}
    for table in tables:
        ddl_map[table] = get_ddl(table)
    return ddl_map

@op(ins={"ddl_map": In(dict)})
def create_embeddings(context, ddl_map):
    """Op: T·∫°o embeddings v√† l∆∞u Qdrant"""
    for table, ddl in ddl_map.items():
        embedding = ollama_embed(ddl)
        qdrant_upsert(table, embedding, ddl)
        context.log.info(f"Embedded: {table}")

@job
def schema_import_job():
    """Job: Full pipeline import schema"""
    tables = extract_tables()
    ddl = generate_ddl(tables)
    create_embeddings(ddl)
```

## 2.3. Resources ‚Äî K·∫øt n·ªëi services

**Resource** = k·∫øt n·ªëi ƒë·∫øn service b√™n ngo√†i (DB, API, etc.), inject v√†o ops/assets.

```python
from dagster import resource, ConfigurableResource
from sqlalchemy import create_engine
from qdrant_client import QdrantClient
import ollama as ollama_sdk

class SourceDatabaseResource(ConfigurableResource):
    """Resource: K·∫øt n·ªëi database ngu·ªìn c·ªßa user"""
    connection_string: str
    
    def get_engine(self):
        return create_engine(self.connection_string)
    
    def execute_query(self, sql: str):
        engine = self.get_engine()
        with engine.connect() as conn:
            from sqlalchemy import text
            result = conn.execute(text(sql))
            return result.fetchall()

class OllamaResource(ConfigurableResource):
    """Resource: Ollama LLM + Embedding"""
    base_url: str = "http://ollama:11434"
    model: str = "llama3.2"
    embed_model: str = "nomic-embed-text"
    
    def get_client(self):
        return ollama_sdk.Client(host=self.base_url)
    
    def embed(self, text: str) -> list[float]:
        client = self.get_client()
        response = client.embed(model=self.embed_model, input=text)
        return response["embeddings"][0]
    
    def generate(self, prompt: str) -> str:
        client = self.get_client()
        response = client.generate(model=self.model, prompt=prompt)
        return response["response"]

class QdrantResource(ConfigurableResource):
    """Resource: Qdrant Vector Store"""
    host: str = "qdrant"
    port: int = 6333
    collection_name: str = "talkwdata_schemas"
    
    def get_client(self):
        return QdrantClient(host=self.host, port=self.port)
    
    def upsert(self, id: str, vector: list[float], payload: dict):
        client = self.get_client()
        from qdrant_client.models import PointStruct
        client.upsert(
            collection_name=self.collection_name,
            points=[PointStruct(id=id, vector=vector, payload=payload)]
        )
    
    def search(self, vector: list[float], limit: int = 5):
        client = self.get_client()
        return client.query_points(
            collection_name=self.collection_name,
            query=vector,
            limit=limit
        )
```

## 2.4. Sensors ‚Äî Trigger t·ª± ƒë·ªông

**Sensor** = function ch·∫°y li√™n t·ª•c, ph√°t hi·ªán s·ª± ki·ªán ‚Üí trigger job/asset.

```python
from dagster import sensor, RunRequest, SensorEvaluationContext

@sensor(job=schema_import_job, minimum_interval_seconds=60)
def new_database_sensor(context: SensorEvaluationContext):
    """Sensor: Ph√°t hi·ªán khi c√≥ database m·ªõi ƒë∆∞·ª£c th√™m ‚Üí trigger import"""
    # Ki·ªÉm tra b·∫£ng database_connections xem c√≥ row m·ªõi kh√¥ng
    last_cursor = context.cursor or "0"
    new_connections = check_new_connections(since_id=int(last_cursor))
    
    for conn in new_connections:
        yield RunRequest(
            run_key=f"import_{conn.id}",
            run_config={
                "resources": {
                    "source_db": {
                        "config": {
                            "connection_string": conn.connection_string
                        }
                    }
                }
            }
        )
    
    if new_connections:
        context.update_cursor(str(new_connections[-1].id))
```

## 2.5. Schedules ‚Äî Ch·∫°y ƒë·ªãnh k·ª≥

```python
from dagster import schedule, ScheduleEvaluationContext

@schedule(cron_schedule="0 */6 * * *", job=schema_sync_job)
def schema_sync_schedule(context: ScheduleEvaluationContext):
    """Schedule: ƒê·ªìng b·ªô schema m·ªói 6 gi·ªù"""
    return RunRequest()

@schedule(cron_schedule="0 0 * * *", job=analytics_job)
def daily_analytics_schedule(context: ScheduleEvaluationContext):
    """Schedule: Th·ªëng k√™ h√†ng ng√†y l√∫c 00:00"""
    return RunRequest()
```

## 2.6. Partitions ‚Äî X·ª≠ l√Ω data theo ph√¢n v√πng

```python
from dagster import DailyPartitionsDefinition, asset

daily_partitions = DailyPartitionsDefinition(start_date="2026-01-01")

@asset(partitions_def=daily_partitions)
def daily_query_stats(context):
    """Asset: Th·ªëng k√™ query theo ng√†y"""
    date = context.partition_key  # "2026-03-01"
    stats = aggregate_query_stats(date)
    return stats
```

---

# Ph·∫ßn 3: Ki·∫øn tr√∫c Dagster trong TalkingWithData

## 3.1. C·∫•u tr√∫c Docker hi·ªán t·∫°i

TalkingWithData ch·∫°y **3 container** Dagster:

| Container | Vai tr√≤ | Port | Entrypoint |
|-----------|---------|------|------------|
| `talkwdata_dagster_webserver` | UI + REST API | 3000 | `dagster-webserver` |
| `talkwdata_dagster_daemon` | Scheduler, Sensors, Run Queue | ‚Äî | `dagster-daemon run` |
| `talkwdata_dagster_orchestration` | Code Server (ch·ª©a pipeline code) | 4000 (gRPC) | `dagster api grpc` |

```
dagster_webserver ‚Üê‚Üí postgres ‚Üê‚Üí dagster_daemon
        ‚Üï (gRPC)                      ‚Üï (gRPC)
    dagster_orchestration         dagster_orchestration
    (code server :4000)           (code server :4000)
```

**T·∫°i sao t√°ch 3 container?**
- **Webserver**: UI lu√¥n s·∫µn s√†ng, kh√¥ng b·ªã ·∫£nh h∆∞·ªüng khi deploy code m·ªõi
- **Daemon**: Ch·∫°y background, qu·∫£n l√Ω schedules/sensors, queue runs
- **Code Server**: Ch·ª©a code pipeline, c√≥ th·ªÉ restart/deploy l·∫°i m√† kh√¥ng ·∫£nh h∆∞·ªüng UI

## 3.2. C·∫•u tr√∫c source code

```
orchestration/
‚îú‚îÄ‚îÄ dagster.yaml                 ‚Üê C·∫•u h√¨nh instance (storage, launcher, ...)
‚îú‚îÄ‚îÄ workspace.yaml               ‚Üê Khai b√°o code locations (gRPC server)
‚îú‚îÄ‚îÄ orchestration.env.example    ‚Üê Bi·∫øn m√¥i tr∆∞·ªùng m·∫´u
‚îú‚îÄ‚îÄ pyproject.toml               ‚Üê Dependencies (dagster, dagster-postgres, ...)
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ orchestration/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ definitions.py       ‚Üê Entry point: load t·∫•t c·∫£ defs
        ‚îî‚îÄ‚îÄ defs/                ‚Üê Th∆∞ m·ª•c ch·ª©a definitions
            ‚îú‚îÄ‚îÄ __init__.py
            ‚îú‚îÄ‚îÄ assets/          ‚Üê (t·∫°o m·ªõi) Data assets
            ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
            ‚îÇ   ‚îú‚îÄ‚îÄ schema_assets.py
            ‚îÇ   ‚îú‚îÄ‚îÄ embedding_assets.py
            ‚îÇ   ‚îú‚îÄ‚îÄ training_assets.py
            ‚îÇ   ‚îî‚îÄ‚îÄ analytics_assets.py
            ‚îú‚îÄ‚îÄ jobs/            ‚Üê (t·∫°o m·ªõi) Jobs
            ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
            ‚îÇ   ‚îú‚îÄ‚îÄ schema_import_job.py
            ‚îÇ   ‚îú‚îÄ‚îÄ schema_sync_job.py
            ‚îÇ   ‚îî‚îÄ‚îÄ analytics_job.py
            ‚îú‚îÄ‚îÄ resources/       ‚Üê (t·∫°o m·ªõi) External connections
            ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
            ‚îÇ   ‚îú‚îÄ‚îÄ source_db.py
            ‚îÇ   ‚îú‚îÄ‚îÄ ollama_resource.py
            ‚îÇ   ‚îî‚îÄ‚îÄ qdrant_resource.py
            ‚îú‚îÄ‚îÄ sensors/         ‚Üê (t·∫°o m·ªõi) Event-driven triggers
            ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
            ‚îÇ   ‚îú‚îÄ‚îÄ new_connection_sensor.py
            ‚îÇ   ‚îî‚îÄ‚îÄ schema_change_sensor.py
            ‚îî‚îÄ‚îÄ schedules/       ‚Üê (t·∫°o m·ªõi) Time-based triggers
                ‚îú‚îÄ‚îÄ __init__.py
                ‚îú‚îÄ‚îÄ sync_schedule.py
                ‚îî‚îÄ‚îÄ analytics_schedule.py
```

## 3.3. definitions.py ‚Äî Entry point

File hi·ªán t·∫°i d√πng `load_from_defs_folder` ‚Äî Dagster t·ª± scan th∆∞ m·ª•c `defs/` v√† load t·∫•t c·∫£ assets, jobs, sensors, schedules, resources:

```python
# orchestration/src/orchestration/definitions.py (hi·ªán t·∫°i)
from pathlib import Path
from dagster import definitions, load_from_defs_folder

@definitions
def defs():
    return load_from_defs_folder(path_within_project=Path(__file__).parent)
```

C√°ch n√†y **t·ª± ƒë·ªông ph√°t hi·ªán** m·ªçi definition trong `defs/` ‚Äî kh√¥ng c·∫ßn import th·ªß c√¥ng.

> **L∆∞u √Ω:** `load_from_defs_folder` y√™u c·∫ßu Dagster 1.10+ v√† m·ªói file trong `defs/` ph·∫£i export Dagster objects (assets, jobs, etc.) ·ªü top-level.

---

# Ph·∫ßn 4: Tri·ªÉn khai Data Pipelines

## 4.1. Pipeline 1: Schema Import (core)

Lu·ªìng ch√≠nh khi user k·∫øt n·ªëi database m·ªõi:

```
User th√™m DB connection
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Extract Schema  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Generate         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Store in        ‚îÇ
‚îÇ  (information_   ‚îÇ     ‚îÇ  Embeddings       ‚îÇ     ‚îÇ  Qdrant          ‚îÇ
‚îÇ   schema)        ‚îÇ     ‚îÇ  (Ollama nomic)   ‚îÇ     ‚îÇ  (vector DB)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                                    ‚îÇ
       ‚ñº                                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Save metadata   ‚îÇ                              ‚îÇ  Train Vanna     ‚îÇ
‚îÇ  (PostgreSQL)    ‚îÇ                              ‚îÇ  (DDL + docs)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Assets Implementation

```python
# defs/assets/schema_assets.py

from dagster import asset, AssetExecutionContext, MaterializeResult, MetadataValue
from dagster import Config
from sqlalchemy import create_engine, text, inspect
from typing import Optional
import json


class SchemaImportConfig(Config):
    """Config cho schema import"""
    connection_string: str
    database_name: str
    schema_name: str = "public"


@asset(
    group_name="schema",
    description="Extract raw schema metadata t·ª´ database ngu·ªìn",
    kinds={"postgres"},
)
def raw_schema_metadata(
    context: AssetExecutionContext,
    config: SchemaImportConfig
) -> dict:
    """
    B∆∞·ªõc 1: K·∫øt n·ªëi database ngu·ªìn, extract DDL cho t·∫•t c·∫£ b·∫£ng
    
    Output: {
        "database_name": str,
        "tables": [
            {
                "table_name": str,
                "columns": [{"name": str, "type": str, "nullable": bool, "primary_key": bool}],
                "ddl": str,
                "foreign_keys": [...],
                "indexes": [...]
            }
        ]
    }
    """
    engine = create_engine(config.connection_string)
    inspector = inspect(engine)
    
    tables_data = []
    table_names = inspector.get_table_names(schema=config.schema_name)
    
    context.log.info(f"Found {len(table_names)} tables in {config.database_name}")
    
    for table_name in table_names:
        # Columns
        columns = []
        pk_columns = [col for col in inspector.get_pk_constraint(table_name, schema=config.schema_name).get("constrained_columns", [])]
        
        for col in inspector.get_columns(table_name, schema=config.schema_name):
            columns.append({
                "name": col["name"],
                "type": str(col["type"]),
                "nullable": col.get("nullable", True),
                "primary_key": col["name"] in pk_columns,
                "default": str(col.get("default", "")) if col.get("default") else None
            })
        
        # Foreign keys
        fks = []
        for fk in inspector.get_foreign_keys(table_name, schema=config.schema_name):
            fks.append({
                "constrained_columns": fk["constrained_columns"],
                "referred_table": fk["referred_table"],
                "referred_columns": fk["referred_columns"]
            })
        
        # Indexes
        indexes = []
        for idx in inspector.get_indexes(table_name, schema=config.schema_name):
            indexes.append({
                "name": idx["name"],
                "columns": idx["column_names"],
                "unique": idx.get("unique", False)
            })
        
        # Generate DDL string
        cols_ddl = []
        for col in columns:
            col_def = f"  {col['name']} {col['type']}"
            if col['primary_key']:
                col_def += " PRIMARY KEY"
            if not col['nullable']:
                col_def += " NOT NULL"
            if col['default']:
                col_def += f" DEFAULT {col['default']}"
            cols_ddl.append(col_def)
        
        # FK constraints in DDL
        for fk in fks:
            fk_def = f"  FOREIGN KEY ({', '.join(fk['constrained_columns'])}) REFERENCES {fk['referred_table']}({', '.join(fk['referred_columns'])})"
            cols_ddl.append(fk_def)
        
        ddl = f"CREATE TABLE {table_name} (\n" + ",\n".join(cols_ddl) + "\n);"
        
        tables_data.append({
            "table_name": table_name,
            "columns": columns,
            "ddl": ddl,
            "foreign_keys": fks,
            "indexes": indexes
        })
        
        context.log.info(f"Extracted: {table_name} ({len(columns)} columns, {len(fks)} FKs)")
    
    engine.dispose()
    
    result = {
        "database_name": config.database_name,
        "schema": config.schema_name,
        "tables": tables_data,
        "table_count": len(tables_data)
    }
    
    # Metadata cho UI tracking
    return MaterializeResult(
        metadata={
            "database_name": MetadataValue.text(config.database_name),
            "table_count": MetadataValue.int(len(tables_data)),
            "tables": MetadataValue.json(
                {t["table_name"]: len(t["columns"]) for t in tables_data}
            )
        },
        value=result
    )
```

```python
# defs/assets/embedding_assets.py

from dagster import asset, AssetExecutionContext, MaterializeResult, MetadataValue
import hashlib


@asset(
    group_name="schema",
    deps=["raw_schema_metadata"],
    description="Sinh embedding vectors cho schema v√† l∆∞u v√†o Qdrant",
    kinds={"qdrant", "ollama"},
)
def schema_embeddings(
    context: AssetExecutionContext,
    raw_schema_metadata: dict,
    ollama_resource: "OllamaResource",
    qdrant_resource: "QdrantResource"
) -> dict:
    """
    B∆∞·ªõc 2: T·∫°o embeddings cho m·ªói table DDL ‚Üí l∆∞u Qdrant
    
    M·ªói table DDL ƒë∆∞·ª£c embed th√†nh 1 vector (768 dims).
    Khi user h·ªèi, system s·∫Ω t√¨m table DDL g·∫ßn nh·∫•t (semantic search).
    """
    database_name = raw_schema_metadata["database_name"]
    tables = raw_schema_metadata["tables"]
    
    embedded_count = 0
    
    for table in tables:
        # T·∫°o text ƒë·ªÉ embed (DDL + column descriptions)
        embed_text = f"Database: {database_name}\n{table['ddl']}"
        
        # T·∫°o deterministic ID
        point_id = hashlib.md5(
            f"{database_name}:{table['table_name']}".encode()
        ).hexdigest()
        
        # Sinh embedding vector (768 dims t·ª´ nomic-embed-text)
        vector = ollama_resource.embed(embed_text)
        
        # Payload metadata
        payload = {
            "database_name": database_name,
            "table_name": table["table_name"],
            "ddl": table["ddl"],
            "column_count": len(table["columns"]),
            "columns": [c["name"] for c in table["columns"]],
            "foreign_keys": table["foreign_keys"]
        }
        
        # Upsert v√†o Qdrant
        qdrant_resource.upsert(
            id=point_id,
            vector=vector,
            payload=payload
        )
        
        embedded_count += 1
        context.log.info(f"Embedded: {table['table_name']} (ID: {point_id[:8]}...)")
    
    return MaterializeResult(
        metadata={
            "database_name": MetadataValue.text(database_name),
            "embedded_tables": MetadataValue.int(embedded_count),
            "vector_dimension": MetadataValue.int(768),
            "collection": MetadataValue.text(qdrant_resource.collection_name)
        },
        value={
            "database_name": database_name,
            "embedded_count": embedded_count
        }
    )
```

```python
# defs/assets/training_assets.py

from dagster import asset, AssetExecutionContext, MaterializeResult, MetadataValue


@asset(
    group_name="training",
    deps=["raw_schema_metadata"],
    description="Train Vanna/LangChain v·ªõi DDL v√† documentation",
    kinds={"ollama"},
)
def trained_model(
    context: AssetExecutionContext,
    raw_schema_metadata: dict,
) -> dict:
    """
    B∆∞·ªõc 3: Training Vanna v·ªõi DDL t·ª´ database ngu·ªìn
    
    Training data bao g·ªìm:
    1. DDL (c·∫•u tr√∫c b·∫£ng)
    2. Documentation (m√¥ t·∫£ nghi·ªáp v·ª•, n·∫øu c√≥)
    3. Quan h·ªá gi·ªØa c√°c b·∫£ng (FK)
    """
    database_name = raw_schema_metadata["database_name"]
    tables = raw_schema_metadata["tables"]
    
    # Import Vanna client (lazy import)
    # from shared.vanna_client import train_ddl, train_documentation
    
    trained_items = 0
    
    for table in tables:
        # Train DDL
        # train_ddl(table["ddl"])
        trained_items += 1
        context.log.info(f"Trained DDL: {table['table_name']}")
    
    # Train documentation (m√¥ t·∫£ quan h·ªá)
    relationships = []
    for table in tables:
        for fk in table.get("foreign_keys", []):
            rel = f"Table {table['table_name']}.{', '.join(fk['constrained_columns'])} references {fk['referred_table']}.{', '.join(fk['referred_columns'])}"
            relationships.append(rel)
    
    if relationships:
        doc = f"Database {database_name} relationships:\n" + "\n".join(relationships)
        # train_documentation(doc)
        trained_items += 1
        context.log.info(f"Trained relationships documentation ({len(relationships)} FKs)")
    
    return MaterializeResult(
        metadata={
            "database_name": MetadataValue.text(database_name),
            "trained_tables": MetadataValue.int(len(tables)),
            "trained_relationships": MetadataValue.int(len(relationships)),
            "total_training_items": MetadataValue.int(trained_items)
        },
        value={
            "database_name": database_name,
            "trained_items": trained_items
        }
    )
```

## 4.2. Pipeline 2: Schema Sync (c·∫≠p nh·∫≠t thay ƒë·ªïi)

```
Cron: m·ªói 6 gi·ªù
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Extract Current ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Compare with     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Update changed  ‚îÇ
‚îÇ  Schema          ‚îÇ     ‚îÇ  Stored Schema    ‚îÇ     ‚îÇ  embeddings      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ  Alert (n·∫øu c√≥   ‚îÇ
                         ‚îÇ  breaking change)‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```python
# defs/assets/schema_sync_assets.py

from dagster import asset, AssetExecutionContext, MaterializeResult, MetadataValue
from dagster import Config


class SchemaSyncConfig(Config):
    connection_string: str
    database_name: str


@asset(
    group_name="sync",
    description="Ph√°t hi·ªán thay ƒë·ªïi schema (th√™m/x√≥a/s·ª≠a b·∫£ng/c·ªôt)",
    kinds={"postgres"},
)
def schema_diff(
    context: AssetExecutionContext,
    config: SchemaSyncConfig,
    qdrant_resource: "QdrantResource"
) -> dict:
    """
    So s√°nh schema hi·ªán t·∫°i c·ªßa DB ngu·ªìn v·ªõi schema ƒë√£ l∆∞u trong Qdrant.
    
    Tr·∫£ v·ªÅ danh s√°ch thay ƒë·ªïi:
    - added_tables: b·∫£ng m·ªõi
    - removed_tables: b·∫£ng ƒë√£ x√≥a
    - modified_tables: b·∫£ng c√≥ c·ªôt thay ƒë·ªïi
    """
    from sqlalchemy import create_engine, inspect
    
    # 1. L·∫•y schema hi·ªán t·∫°i t·ª´ DB ngu·ªìn
    engine = create_engine(config.connection_string)
    inspector = inspect(engine)
    current_tables = set(inspector.get_table_names(schema="public"))
    
    current_schema = {}
    for table in current_tables:
        cols = inspector.get_columns(table, schema="public")
        current_schema[table] = {col["name"]: str(col["type"]) for col in cols}
    
    engine.dispose()
    
    # 2. L·∫•y schema ƒë√£ l∆∞u t·ª´ Qdrant metadata
    client = qdrant_resource.get_client()
    stored_points = client.scroll(
        collection_name=qdrant_resource.collection_name,
        scroll_filter={
            "must": [
                {"key": "database_name", "match": {"value": config.database_name}}
            ]
        },
        limit=1000
    )[0]
    
    stored_tables = {p.payload["table_name"] for p in stored_points}
    stored_schema = {}
    for p in stored_points:
        stored_schema[p.payload["table_name"]] = {
            col: "" for col in p.payload.get("columns", [])
        }
    
    # 3. So s√°nh
    added = current_tables - stored_tables
    removed = stored_tables - current_tables
    common = current_tables & stored_tables
    
    modified = {}
    for table in common:
        current_cols = set(current_schema.get(table, {}).keys())
        stored_cols = set(stored_schema.get(table, {}).keys())
        
        new_cols = current_cols - stored_cols
        dropped_cols = stored_cols - current_cols
        
        if new_cols or dropped_cols:
            modified[table] = {
                "added_columns": list(new_cols),
                "removed_columns": list(dropped_cols)
            }
    
    diff = {
        "database_name": config.database_name,
        "added_tables": list(added),
        "removed_tables": list(removed),
        "modified_tables": modified,
        "has_changes": bool(added or removed or modified)
    }
    
    context.log.info(
        f"Schema diff: +{len(added)} tables, -{len(removed)} tables, "
        f"~{len(modified)} modified"
    )
    
    return MaterializeResult(
        metadata={
            "has_changes": MetadataValue.bool(diff["has_changes"]),
            "added_tables": MetadataValue.int(len(added)),
            "removed_tables": MetadataValue.int(len(removed)),
            "modified_tables": MetadataValue.int(len(modified)),
            "details": MetadataValue.json(diff)
        },
        value=diff
    )


@asset(
    group_name="sync",
    deps=["schema_diff"],
    description="C·∫≠p nh·∫≠t embeddings cho c√°c b·∫£ng ƒë√£ thay ƒë·ªïi",
)
def updated_embeddings(
    context: AssetExecutionContext,
    schema_diff: dict,
    ollama_resource: "OllamaResource",
    qdrant_resource: "QdrantResource"
) -> dict:
    """
    Ch·ªâ c·∫≠p nh·∫≠t embeddings cho b·∫£ng thay ƒë·ªïi (incremental update).
    Kh√¥ng re-embed to√†n b·ªô ‚Üí ti·∫øt ki·ªám th·ªùi gian.
    """
    if not schema_diff["has_changes"]:
        context.log.info("No schema changes detected. Skipping.")
        return {"updated": 0, "removed": 0}
    
    updated = 0
    removed = 0
    
    # X√≥a embeddings c·ªßa b·∫£ng ƒë√£ b·ªã x√≥a
    for table in schema_diff["removed_tables"]:
        # qdrant_resource.delete_by_table(schema_diff["database_name"], table)
        removed += 1
        context.log.info(f"Removed embedding: {table}")
    
    # Re-embed b·∫£ng m·ªõi v√† b·∫£ng ƒë√£ s·ª≠a
    tables_to_embed = (
        schema_diff["added_tables"] + 
        list(schema_diff["modified_tables"].keys())
    )
    
    for table_name in tables_to_embed:
        # Re-extract DDL v√† embed (t∆∞∆°ng t·ª± schema_embeddings asset)
        context.log.info(f"Re-embedding: {table_name}")
        updated += 1
    
    return MaterializeResult(
        metadata={
            "tables_updated": MetadataValue.int(updated),
            "tables_removed": MetadataValue.int(removed)
        },
        value={"updated": updated, "removed": removed}
    )
```

## 4.3. Pipeline 3: Analytics & Monitoring

```python
# defs/assets/analytics_assets.py

from dagster import asset, AssetExecutionContext, DailyPartitionsDefinition, MetadataValue, MaterializeResult

daily_partitions = DailyPartitionsDefinition(start_date="2026-01-01")


@asset(
    group_name="analytics",
    partitions_def=daily_partitions,
    description="Th·ªëng k√™ queries h√†ng ng√†y",
    kinds={"postgres"},
)
def daily_query_stats(context: AssetExecutionContext) -> dict:
    """
    Th·ªëng k√™ cho ng√†y partition_key:
    - T·ªïng s·ªë queries
    - S·ªë queries th√†nh c√¥ng (SQL valid + c√≥ k·∫øt qu·∫£)
    - S·ªë queries th·∫•t b·∫°i
    - Query ph·ªï bi·∫øn nh·∫•t
    - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh
    """
    date = context.partition_key  # "2026-03-01"
    
    # Query t·ª´ b·∫£ng messages trong PostgreSQL
    # stats = db.query(...).filter(Message.created_at == date)...
    
    stats = {
        "date": date,
        "total_queries": 0,
        "successful_queries": 0,
        "failed_queries": 0,
        "avg_response_time_ms": 0,
        "unique_users": 0,
        "top_tables_queried": []
    }
    
    context.log.info(f"Analytics for {date}: {stats['total_queries']} queries")
    
    return MaterializeResult(
        metadata={
            "date": MetadataValue.text(date),
            "total_queries": MetadataValue.int(stats["total_queries"]),
            "success_rate": MetadataValue.float(
                stats["successful_queries"] / max(stats["total_queries"], 1) * 100
            ),
            "unique_users": MetadataValue.int(stats["unique_users"])
        },
        value=stats
    )


@asset(
    group_name="analytics",
    deps=["daily_query_stats"],
    description="Ph√¢n t√≠ch accuracy c·ªßa SQL generation",
)
def sql_accuracy_report(
    context: AssetExecutionContext,
    daily_query_stats: dict
) -> dict:
    """
    B√°o c√°o ch·∫•t l∆∞·ª£ng SQL generation:
    - % SQL valid (parse ƒë∆∞·ª£c)
    - % SQL th·ª±c thi th√†nh c√¥ng
    - L·ªói ph·ªï bi·∫øn nh·∫•t
    - B·∫£ng hay b·ªã truy v·∫•n sai
    """
    report = {
        "total_queries": daily_query_stats.get("total_queries", 0),
        "sql_parse_success_rate": 0.0,
        "sql_execution_success_rate": 0.0,
        "common_errors": [],
        "problematic_tables": []
    }
    
    return MaterializeResult(
        metadata={
            "parse_success_rate": MetadataValue.float(report["sql_parse_success_rate"]),
            "execution_success_rate": MetadataValue.float(report["sql_execution_success_rate"])
        },
        value=report
    )
```

---

# Ph·∫ßn 5: Sensors & Schedules ‚Äî Automation

## 5.1. Sensor: Ph√°t hi·ªán database m·ªõi

```python
# defs/sensors/new_connection_sensor.py

from dagster import sensor, RunRequest, SensorEvaluationContext, SensorResult, SkipReason
import requests


@sensor(
    description="Ph√°t hi·ªán khi user th√™m database connection m·ªõi ‚Üí trigger schema import",
    minimum_interval_seconds=30
)
def new_connection_sensor(context: SensorEvaluationContext) -> SensorResult:
    """
    Polling API server m·ªói 30 gi√¢y:
    - GET /text-to-data/connections?since={cursor}
    - N·∫øu c√≥ connection m·ªõi ‚Üí RunRequest cho schema import pipeline
    """
    last_checked_id = int(context.cursor) if context.cursor else 0
    
    try:
        # G·ªçi FastAPI server
        response = requests.get(
            "http://server:8000/text-to-data/connections",
            params={"since_id": last_checked_id},
            timeout=10
        )
        
        if response.status_code != 200:
            return SkipReason(f"API returned {response.status_code}")
        
        new_connections = response.json().get("connections", [])
        
        if not new_connections:
            return SkipReason("No new connections")
        
        run_requests = []
        max_id = last_checked_id
        
        for conn in new_connections:
            run_requests.append(
                RunRequest(
                    run_key=f"schema_import_{conn['id']}_{conn['database_name']}",
                    run_config={
                        "ops": {
                            "raw_schema_metadata": {
                                "config": {
                                    "connection_string": conn["connection_string"],
                                    "database_name": conn["database_name"]
                                }
                            }
                        }
                    },
                    tags={
                        "database_name": conn["database_name"],
                        "trigger": "new_connection_sensor"
                    }
                )
            )
            max_id = max(max_id, conn["id"])
        
        context.update_cursor(str(max_id))
        
        return SensorResult(
            run_requests=run_requests,
            cursor=str(max_id)
        )
    
    except requests.RequestException as e:
        return SkipReason(f"Cannot reach server: {str(e)}")
```

## 5.2. Sensor: Ph√°t hi·ªán schema thay ƒë·ªïi

```python
# defs/sensors/schema_change_sensor.py

from dagster import sensor, RunRequest, SensorEvaluationContext, SkipReason
from sqlalchemy import create_engine, inspect
import json


@sensor(
    description="Ki·ªÉm tra schema DB ngu·ªìn c√≥ thay ƒë·ªïi kh√¥ng (m·ªói 5 ph√∫t)",
    minimum_interval_seconds=300  # 5 ph√∫t
)
def schema_change_sensor(context: SensorEvaluationContext):
    """
    So s√°nh fingerprint c·ªßa schema hi·ªán t·∫°i v·ªõi l·∫ßn check tr∆∞·ªõc.
    N·∫øu kh√°c ‚Üí trigger schema sync pipeline.
    
    Fingerprint = hash(sorted table names + column names)
    """
    import hashlib
    
    # L·∫•y danh s√°ch connections c·∫ßn monitor
    # connections = get_active_connections()
    connections = []  # Placeholder
    
    cursor_data = json.loads(context.cursor) if context.cursor else {}
    
    for conn in connections:
        try:
            engine = create_engine(conn["connection_string"])
            inspector = inspect(engine)
            tables = sorted(inspector.get_table_names(schema="public"))
            
            # T·∫°o fingerprint
            schema_parts = []
            for table in tables:
                cols = sorted([c["name"] for c in inspector.get_columns(table)])
                schema_parts.append(f"{table}:{','.join(cols)}")
            
            fingerprint = hashlib.sha256("|".join(schema_parts).encode()).hexdigest()
            engine.dispose()
            
            # So s√°nh v·ªõi l·∫ßn tr∆∞·ªõc
            db_key = conn["database_name"]
            prev_fingerprint = cursor_data.get(db_key)
            
            if prev_fingerprint and prev_fingerprint != fingerprint:
                context.log.info(f"Schema change detected in {db_key}")
                yield RunRequest(
                    run_key=f"sync_{db_key}_{fingerprint[:8]}",
                    run_config={
                        "ops": {
                            "schema_diff": {
                                "config": {
                                    "connection_string": conn["connection_string"],
                                    "database_name": db_key
                                }
                            }
                        }
                    },
                    tags={"trigger": "schema_change_sensor", "database": db_key}
                )
            
            cursor_data[db_key] = fingerprint
        
        except Exception as e:
            context.log.warning(f"Error checking {conn.get('database_name', '?')}: {e}")
    
    context.update_cursor(json.dumps(cursor_data))
```

## 5.3. Schedule: ƒê·ªãnh k·ª≥ sync

```python
# defs/schedules/sync_schedule.py

from dagster import schedule, ScheduleEvaluationContext, RunRequest


@schedule(
    cron_schedule="0 */6 * * *",  # M·ªói 6 gi·ªù
    description="ƒê·ªìng b·ªô schema m·ªói 6 gi·ªù cho t·∫•t c·∫£ database ƒë√£ k·∫øt n·ªëi"
)
def schema_sync_schedule(context: ScheduleEvaluationContext):
    """
    Ch·∫°y l√∫c: 00:00, 06:00, 12:00, 18:00
    Trigger schema sync cho t·∫•t c·∫£ active connections.
    """
    # connections = get_all_active_connections()
    connections = []  # Placeholder
    
    for conn in connections:
        yield RunRequest(
            run_key=f"scheduled_sync_{conn['database_name']}_{context.scheduled_execution_time.isoformat()}",
            run_config={
            "ops": {
                    "schema_diff": {
                        "config": {
                            "connection_string": conn["connection_string"],
                            "database_name": conn["database_name"]
                        }
                    }
                }
            },
            tags={
                "trigger": "schema_sync_schedule",
                "scheduled_time": context.scheduled_execution_time.isoformat()
            }
        )
```

```python
# defs/schedules/analytics_schedule.py

from dagster import schedule, ScheduleEvaluationContext, RunRequest


@schedule(
    cron_schedule="30 0 * * *",  # M·ªói ng√†y l√∫c 00:30
    description="T·∫°o b√°o c√°o analytics h√†ng ng√†y"
)
def daily_analytics_schedule(context: ScheduleEvaluationContext):
    """
    Ch·∫°y l√∫c 00:30 m·ªói ng√†y.
    T·∫°o th·ªëng k√™ query ng√†y h√¥m tr∆∞·ªõc.
    """
    yesterday = (
        context.scheduled_execution_time.date() 
        - __import__('datetime').timedelta(days=1)
    )
    
    return RunRequest(
        run_key=f"analytics_{yesterday.isoformat()}",
        tags={
            "trigger": "daily_analytics_schedule",
            "date": yesterday.isoformat()
        }
    )
```

---

# Ph·∫ßn 6: Tracking & Observability

## 6.1. Dagster UI ‚Äî T·ªïng quan

Truy c·∫≠p: **http://localhost:3000** (dagster_webserver)

### C√°c tab ch√≠nh:

| Tab | Ch·ª©c nƒÉng |
|-----|----------|
| **Asset Catalog** | Xem t·∫•t c·∫£ assets, dependency graph, materialization history |
| **Runs** | L·ªãch s·ª≠ t·∫•t c·∫£ pipeline runs, status, duration, logs |
| **Jobs** | Danh s√°ch jobs, trigger th·ªß c√¥ng |
| **Schedules** | Qu·∫£n l√Ω schedules (b·∫≠t/t·∫Øt, xem history) |
| **Sensors** | Qu·∫£n l√Ω sensors (b·∫≠t/t·∫Øt, xem ticks) |
| **Resources** | Xem configured resources |

### Asset Graph Visualization

```
Dagster UI t·ª± ƒë·ªông v·∫Ω dependency graph:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ raw_schema_metadata‚îÇ ‚Üê Group: schema
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
    ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇschema_ ‚îÇ  ‚îÇtrained_  ‚îÇ ‚Üê Group: training
‚îÇembeddings‚îÇ  ‚îÇmodel     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇschema_diff ‚îÇ ‚Üê Group: sync
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇupdated_      ‚îÇ
‚îÇembeddings    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇdaily_query_stats‚îÇ ‚Üê Group: analytics (partitioned)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇsql_accuracy_report ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 6.2. Run Tracking

M·ªói pipeline run ƒë∆∞·ª£c Dagster ghi l·∫°i ƒë·∫ßy ƒë·ªß:

```
Run ID:     a1b2c3d4-5678-...
Status:     ‚úÖ SUCCESS / ‚ùå FAILURE / üîÑ IN_PROGRESS
Start:      2026-03-01 10:00:00
End:        2026-03-01 10:02:35
Duration:   2m 35s
Tags:       trigger=new_connection_sensor, database=ecommerce_db
```

### Structured Logs

```python
# Trong ops/assets, d√πng context.log
context.log.info(f"Processing table: {table_name}")
context.log.warning(f"Slow query detected: {duration}ms")
context.log.error(f"Failed to connect: {error}")

# Logs hi·ªÉn th·ªã trong UI v·ªõi timestamps, severity levels
```

### Metadata Tracking

```python
# Metadata ƒë∆∞·ª£c hi·ªÉn th·ªã trong Asset Catalog
return MaterializeResult(
    metadata={
        # Hi·ªÉn th·ªã d·∫°ng text
        "database": MetadataValue.text("ecommerce_db"),
        
        # Hi·ªÉn th·ªã d·∫°ng s·ªë
        "table_count": MetadataValue.int(15),
        
        # Hi·ªÉn th·ªã d·∫°ng JSON (expandable)
        "details": MetadataValue.json({"tables": ["orders", "customers"]}),
        
        # Hi·ªÉn th·ªã d·∫°ng markdown
        "summary": MetadataValue.md("## Report\n- 15 tables\n- 120 columns"),
        
        # Hi·ªÉn th·ªã progress
        "success_rate": MetadataValue.float(95.5),
        
        # Link
        "dashboard": MetadataValue.url("http://localhost:3000/assets"),
    }
)
```

## 6.3. Alerting (khi pipeline l·ªói)

```python
# defs/resources/alerting.py

from dagster import failure_hook, success_hook, HookContext
import requests


@failure_hook
def notify_on_failure(context: HookContext):
    """Hook: G·ª≠i th√¥ng b√°o khi pipeline th·∫•t b·∫°i"""
    message = (
        f"üî¥ Pipeline Failed!\n"
        f"Op: {context.op.name}\n"
        f"Run ID: {context.run_id}\n"
        f"Error: {context.op_exception}"
    )
    
    context.log.error(message)
    
    # G·ª≠i webhook (Slack, Discord, etc.)
    # requests.post(WEBHOOK_URL, json={"text": message})


@success_hook
def log_on_success(context: HookContext):
    """Hook: Log khi pipeline th√†nh c√¥ng"""
    context.log.info(f"‚úÖ Op {context.op.name} completed successfully")


# S·ª≠ d·ª•ng hook trong job
from dagster import job

@job(hooks={notify_on_failure, log_on_success})
def schema_import_job():
    ...
```

## 6.4. Data Lineage (theo d√µi ngu·ªìn g·ªëc data)

Dagster t·ª± ƒë·ªông tracking **data lineage** ‚Äî bi·∫øt m·ªói asset ƒë∆∞·ª£c t·∫°o t·ª´ ƒë√¢u:

```
Lineage cho "schema_embeddings":
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Upstream:
  ‚îî‚îÄ‚îÄ raw_schema_metadata (last materialized: 2026-03-01 10:00)
      ‚îî‚îÄ‚îÄ Source: postgres://user@host/ecommerce_db

Downstream:
  ‚îî‚îÄ‚îÄ trained_model (stale - c·∫ßn re-materialize)
```

**Stale detection**: Dagster t·ª± bi·∫øt khi upstream asset thay ƒë·ªïi ‚Üí downstream tr·ªü n√™n "stale" v√† c·∫ßn c·∫≠p nh·∫≠t.

## 6.5. Monitoring Queries

Dagster l∆∞u m·ªçi th·ª© v√†o PostgreSQL. C√≥ th·ªÉ query tr·ª±c ti·∫øp:

```sql
-- Xem run history
SELECT run_id, status, start_time, end_time, 
       end_time - start_time as duration
FROM runs 
ORDER BY start_time DESC 
LIMIT 20;

-- Xem th·ªëng k√™ theo status
SELECT status, COUNT(*) as count
FROM runs
WHERE start_time > NOW() - INTERVAL '7 days'
GROUP BY status;

-- Xem materialization history cho 1 asset
SELECT * FROM event_logs
WHERE dagster_event_type = 'ASSET_MATERIALIZATION'
  AND asset_key = 'schema_embeddings'
ORDER BY timestamp DESC;
```

---

# Ph·∫ßn 7: C·∫•u h√¨nh & Deployment

## 7.1. dagster.yaml ‚Äî Chi ti·∫øt

File c·∫•u h√¨nh hi·ªán t·∫°i c·ªßa d·ª± √°n:

```yaml
# orchestration/dagster.yaml

# ========================================
# Storage: L∆∞u run history, event logs
# ========================================
storage:
  postgres:
    postgres_db:
      username:
        env: DAGSTER_PG_USERNAME      # dagster
      password:
        env: DAGSTER_PG_PASSWORD      # dagster_password
      hostname:
        env: DAGSTER_PG_HOSTNAME      # postgres
      db_name:
        env: DAGSTER_PG_DB            # dagster
      port: 5432

# ========================================
# Run Launcher: C√°ch launch m·ªói pipeline run
# ========================================
run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher
  # DefaultRunLauncher: ch·∫°y run trong c√πng process
  # C√≥ th·ªÉ thay b·∫±ng:
  # - DockerRunLauncher (m·ªói run = 1 container)
  # - K8sRunLauncher (m·ªói run = 1 pod)

# ========================================
# Run Coordinator: Qu·∫£n l√Ω h√†ng ƒë·ª£i runs
# ========================================
run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator        # H√†ng ƒë·ª£i FIFO
  config:
    max_concurrent_runs:
      env: DAGSTER_OVERALL_CONCURRENCY_LIMIT  # 10

# ========================================
# Compute Logs: Stdout/stderr logs
# ========================================
compute_logs:
  module: dagster.core.storage.local_compute_log_manager
  class: LocalComputeLogManager
  config:
    base_dir: /opt/dagster/dagster_home/compute_logs

# ========================================
# Local Artifact Storage
# ========================================
local_artifact_storage:
  module: dagster.core.storage.root
  class: LocalArtifactStorage
  config:
    base_dir: /opt/dagster/dagster_home/local_artifact_storage

# ========================================
# Telemetry & Threading
# ========================================
telemetry:
  enabled: true

sensors:
  use_threads: true
  num_workers: 3         # 3 threads cho sensors

schedules:
  use_threads: true
  num_workers: 3         # 3 threads cho schedules
```

## 7.2. workspace.yaml ‚Äî Code Locations

```yaml
# orchestration/workspace.yaml
load_from:
  - grpc_server:
      host: dagster_orchestration    # Container name
      port: 4000                     # gRPC port
      location_name: "orchestration"
```

**Gi·∫£i th√≠ch:** Webserver v√† Daemon k·∫øt n·ªëi t·ªõi Code Server qua gRPC. Code Server ch·ª©a to√†n b·ªô pipeline code (definitions.py + defs/).

## 7.3. Docker containers

### Container 1: dagster_webserver

```yaml
# Entrypoint
entrypoint:
  - dagster-webserver
  - -h "0.0.0.0"
  - -p "3000"
  - -w /opt/dagster/dagster_home/workspace.yaml

# Volumes
volumes:
  - ./orchestration/dagster.yaml:/opt/dagster/dagster_home/dagster.yaml
  - ./orchestration/workspace.yaml:/opt/dagster/dagster_home/workspace.yaml
  - ./volumes/dagster_home:/opt/dagster/dagster_home
```

### Container 2: dagster_daemon

```yaml
# Entrypoint
working_dir: /opt/dagster/dagster_home
entrypoint:
  - dagster-daemon
  - run

# Daemon ch·∫°y:
# - SchedulerDaemon: tick schedules theo cron
# - SensorDaemon: tick sensors theo interval
# - QueuedRunCoordinatorDaemon: dequeue runs
```

### Container 3: dagster_orchestration (Code Server)

```yaml
# Entrypoint
entrypoint:
  - dagster api grpc
  - -h "0.0.0.0"
  - -p "4000"
  - -m orchestration.definitions    # Module ch·ª©a @definitions
```

## 7.4. Bi·∫øn m√¥i tr∆∞·ªùng

```bash
# orchestration/.env (copy t·ª´ orchestration.env.example)

# PostgreSQL cho Dagster storage
DAGSTER_PG_USERNAME=dagster
DAGSTER_PG_PASSWORD=dagster_password
DAGSTER_PG_HOSTNAME=postgres
DAGSTER_PG_PORT=5432
DAGSTER_PG_DB=dagster

# Concurrency
DAGSTER_OVERALL_CONCURRENCY_LIMIT=10

# (Th√™m n·∫øu c·∫ßn) K·∫øt n·ªëi TalkingWithData services
# TALKWDATA_SERVER_URL=http://server:8000
# OLLAMA_BASE_URL=http://ollama:11434
# QDRANT_HOST=qdrant
# QDRANT_PORT=6333
```

---

# Ph·∫ßn 8: Best Practices & Troubleshooting

## 8.1. Best Practices

### Naming Convention

```python
# Assets: danh t·ª´ (m√¥ t·∫£ data)
@asset
def raw_schema_metadata(): ...    # ‚úÖ noun
def extract_schema(): ...          # ‚ùå verb

# Ops: ƒë·ªông t·ª´ (m√¥ t·∫£ h√†nh ƒë·ªông)
@op
def extract_tables(): ...          # ‚úÖ verb
def tables(): ...                  # ‚ùå noun

# Groups: theo domain
group_name="schema"      # Schema management assets
group_name="training"    # ML training assets
group_name="sync"        # Sync/update assets
group_name="analytics"   # Reporting assets
```

### Idempotency (ch·∫°y l·∫°i an to√†n)

```python
@asset
def schema_embeddings(context, raw_schema_metadata):
    """
    IDEMPOTENT: D√πng upsert thay v√¨ insert.
    Ch·∫°y l·∫°i N l·∫ßn ‚Üí k·∫øt qu·∫£ gi·ªëng nhau.
    """
    for table in raw_schema_metadata["tables"]:
        # ‚úÖ Upsert (update n·∫øu t·ªìn t·∫°i, insert n·∫øu ch∆∞a)
        qdrant_resource.upsert(id=table_id, vector=vector, payload=payload)
        
        # ‚ùå Insert (ch·∫°y l·∫°i ‚Üí duplicate)
        # qdrant_resource.insert(vector=vector, payload=payload)
```

### Error Handling

```python
from dagster import Failure, RetryPolicy

@asset(
    retry_policy=RetryPolicy(
        max_retries=3,
        delay=10  # seconds between retries
    )
)
def fragile_asset(context):
    try:
        result = call_external_api()
        return result
    except ConnectionError as e:
        # Dagster s·∫Ω retry 3 l·∫ßn
        raise Failure(
            description=f"API connection failed: {e}",
            metadata={"error": str(e)}
        )
    except ValueError as e:
        # Kh√¥ng retry ‚Äî l·ªói logic
        raise Failure(
            description=f"Invalid data: {e}",
            metadata={"error": str(e)},
        )
```

### Resource Cleanup

```python
from contextlib import contextmanager

class SourceDatabaseResource(ConfigurableResource):
    connection_string: str
    
    @contextmanager
    def get_connection(self):
        """Context manager ƒë·∫£m b·∫£o connection ƒë∆∞·ª£c ƒë√≥ng"""
        engine = create_engine(self.connection_string)
        conn = engine.connect()
        try:
            yield conn
        finally:
            conn.close()
            engine.dispose()

# S·ª≠ d·ª•ng
@asset
def my_asset(source_db: SourceDatabaseResource):
    with source_db.get_connection() as conn:
        result = conn.execute(text("SELECT * FROM ..."))
```

## 8.2. Testing

```python
# tests/test_assets.py

from dagster import materialize
from orchestration.defs.assets.schema_assets import raw_schema_metadata


def test_raw_schema_metadata():
    """Test asset v·ªõi mock config"""
    result = materialize(
        [raw_schema_metadata],
        run_config={
            "ops": {
                "raw_schema_metadata": {
                    "config": {
                        "connection_string": "postgresql://test:test@localhost:5432/test_db",
                        "database_name": "test_db"
                    }
                }
            }
        }
    )
    
    assert result.success
    
    # Ki·ªÉm tra output
    output = result.output_for_node("raw_schema_metadata")
    assert "tables" in output
    assert output["database_name"] == "test_db"


def test_schema_diff_no_changes():
    """Test schema diff khi kh√¥ng c√≥ thay ƒë·ªïi"""
    # Mock resources
    from unittest.mock import MagicMock
    
    mock_qdrant = MagicMock()
    mock_qdrant.get_client.return_value.scroll.return_value = ([], None)
    
    result = materialize(
        [schema_diff],
        resources={"qdrant_resource": mock_qdrant},
        run_config={...}
    )
    
    assert result.success
    output = result.output_for_node("schema_diff")
    assert output["has_changes"] == False
```

## 8.3. Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p

| L·ªói | Nguy√™n nh√¢n | Gi·∫£i ph√°p |
|-----|-------------|-----------|
| `Could not connect to gRPC server` | Code Server ch∆∞a kh·ªüi ƒë·ªông xong | ƒê·ª£i container `dagster_orchestration` healthy |
| `DagsterEventLogInvalidForRun` | PostgreSQL storage l·ªói | Restart dagster_webserver, ki·ªÉm tra disk space |
| `RunQueueFull` | Qu√° nhi·ªÅu runs ƒëang ch·ªù | TƒÉng `DAGSTER_OVERALL_CONCURRENCY_LIMIT` |
| `ModuleNotFoundError` | Code ch∆∞a ƒë∆∞·ª£c install trong container | Rebuild image: `docker compose build dagster_orchestration` |
| `Sensor tick failed` | API server kh√¥ng reachable | Ki·ªÉm tra network gi·ªØa containers |

### Debug commands

```bash
# Xem logs webserver
docker logs talkwdata_dagster_webserver --tail 100

# Xem logs daemon (schedules, sensors)
docker logs talkwdata_dagster_daemon --tail 100

# Xem logs code server
docker logs talkwdata_dagster_orchestration --tail 100

# Restart code server (sau khi update code)
docker compose restart dagster_orchestration

# Rebuild (sau khi th√™m dependencies)
docker compose build dagster_orchestration
docker compose up -d dagster_orchestration dagster_webserver dagster_daemon

# Ki·ªÉm tra gRPC connection
docker exec talkwdata_dagster_webserver dagster api grpc-health-check -p 4000 -h dagster_orchestration

# Ch·∫°y pipeline c·ª•c b·ªô (dev mode)
cd orchestration
dagster dev -m orchestration.definitions
```

### Ch·∫°y local (development)

```bash
# C√†i ƒë·∫∑t
cd orchestration
pip install -e ".[dev]"

# Kh·ªüi ƒë·ªông Dagster dev (g·ªôp webserver + daemon + code server)
dagster dev -m orchestration.definitions

# M·ªü browser: http://localhost:3000
```

## 8.4. Flow tracking t·ªïng h·ª£p

D∆∞·ªõi ƒë√¢y l√† **full picture** v·ªÅ c√°ch Dagster tracking m·ªçi data flow trong TalkingWithData:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DAGSTER TRACKING                              ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  üìä Asset Catalog                                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ schema/raw_schema_metadata     [‚úÖ Fresh   | 2026-03-01 10:00] ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ schema/schema_embeddings       [‚úÖ Fresh   | 2026-03-01 10:02] ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ training/trained_model         [‚ö†Ô∏è Stale   | 2026-02-28 08:00] ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sync/schema_diff               [‚úÖ Fresh   | 2026-03-01 12:00] ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ sync/updated_embeddings        [‚úÖ Fresh   | 2026-03-01 12:01] ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ analytics/daily_query_stats    [üìÖ 15/15 partitions]            ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  üîÑ Recent Runs                                                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Run a1b2c3d4 | schema_import  | ‚úÖ SUCCESS  | 2m 35s           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Run e5f6g7h8 | schema_sync    | ‚úÖ SUCCESS  | 45s              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Run i9j0k1l2 | analytics      | ‚ùå FAILURE  | 12s (retry 1/3) ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Run m3n4o5p6 | schema_import  | üîÑ RUNNING  | 1m 20s...       ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚è∞ Schedules                                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ schema_sync_schedule     | */6h  | Next: 18:00 | ‚úÖ ON          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ daily_analytics_schedule | 00:30 | Next: 00:30 | ‚úÖ ON          ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  üëÅ Sensors                                                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ new_connection_sensor    | 30s   | Last: 5 min ago | ‚úÖ ON      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ schema_change_sensor    | 5min  | Last: 3 min ago | ‚úÖ ON      ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  üìà Metadata (per asset materialization)                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ schema_embeddings [Run a1b2c3d4]:                               ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ database_name: "ecommerce_db"                               ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ embedded_tables: 15                                         ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ vector_dimension: 768                                       ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ collection: "talkwdata_schemas"                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**M·ªçi th·ª© ƒë·ªÅu observable qua UI t·∫°i http://localhost:3000** ‚Äî kh√¥ng c·∫ßn ƒë·ªçc log th·ªß c√¥ng.
