# LangChain & Vanna 2.0 ‚Äî H∆∞·ªõng d·∫´n Chi ti·∫øt

> **Ng·ªØ c·∫£nh:** √Åp d·ª•ng trong d·ª± √°n TalkingWithData (Text-to-SQL v·ªõi Ollama local)  
> **C·∫≠p nh·∫≠t:** 01/03/2026

---

## M·ª•c l·ª•c

- [Ph·∫ßn 1: LangChain ‚Äî T·ªïng quan & Ki·∫øn tr√∫c](#ph·∫ßn-1-langchain--t·ªïng-quan--ki·∫øn-tr√∫c)
- [Ph·∫ßn 2: LangChain ‚Äî C√°ch s·ª≠ d·ª•ng Chi ti·∫øt](#ph·∫ßn-2-langchain--c√°ch-s·ª≠-d·ª•ng-chi-ti·∫øt)
- [Ph·∫ßn 3: Vanna 2.0 ‚Äî Text-to-SQL chuy√™n bi·ªát](#ph·∫ßn-3-vanna-20--text-to-sql-chuy√™n-bi·ªát)
- [Ph·∫ßn 4: So s√°nh LangChain vs Vanna 2.0](#ph·∫ßn-4-so-s√°nh-langchain-vs-vanna-20)
- [Ph·∫ßn 5: √Åp d·ª•ng v√†o TalkingWithData](#ph·∫ßn-5-√°p-d·ª•ng-v√†o-talkingwithdata)

---

# Ph·∫ßn 1: LangChain ‚Äî T·ªïng quan & Ki·∫øn tr√∫c

## 1.1. LangChain l√† g√¨?

LangChain l√† m·ªôt **framework m√£ ngu·ªìn m·ªü** (Python/JS) ƒë·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng s·ª≠ d·ª•ng Large Language Models (LLMs). N√≥ cung c·∫•p c√°c abstraction layer gi√∫p:

- K·∫øt n·ªëi LLM v·ªõi d·ªØ li·ªáu b√™n ngo√†i (databases, APIs, documents)
- X√¢y d·ª±ng chu·ªói x·ª≠ l√Ω (chains) ph·ª©c t·∫°p
- Qu·∫£n l√Ω b·ªô nh·ªõ h·ªôi tho·∫°i (memory)
- T·∫°o AI agents c√≥ kh·∫£ nƒÉng s·ª≠ d·ª•ng tools

```
                    LangChain Ecosystem
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇlangchain ‚îÇ  ‚îÇlangchain ‚îÇ  ‚îÇ langchain-     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ-core     ‚îÇ  ‚îÇ-community‚îÇ  ‚îÇ ollama/openai/ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ(base)    ‚îÇ  ‚îÇ(3rd party‚îÇ  ‚îÇ (integrations) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇLangGraph ‚îÇ  ‚îÇLangSmith ‚îÇ  ‚îÇ LangServe     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ(agents,  ‚îÇ  ‚îÇ(tracing, ‚îÇ  ‚îÇ (deploy as    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ workflow) ‚îÇ  ‚îÇ debug)   ‚îÇ  ‚îÇ  REST API)    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1.2. C√°c th√†nh ph·∫ßn ch√≠nh

| Th√†nh ph·∫ßn | M√¥ t·∫£ | V√≠ d·ª• |
|------------|--------|-------|
| **Models** | K·∫øt n·ªëi v·ªõi LLM | ChatOllama, ChatOpenAI |
| **Prompts** | Template qu·∫£n l√Ω prompt | ChatPromptTemplate, FewShotPromptTemplate |
| **Chains** | Chu·ªói x·ª≠ l√Ω tu·∫ßn t·ª± | LLMChain, SequentialChain, LCEL |
| **Memory** | B·ªô nh·ªõ h·ªôi tho·∫°i | ConversationBufferMemory |
| **Retrievers** | Truy xu·∫•t d·ªØ li·ªáu | VectorStoreRetriever |
| **Agents** | LLM t·ª± ch·ªçn tool | SQL Agent, Custom Agent |
| **Tools** | C√¥ng c·ª• cho Agent | SQLDatabaseTool, PythonREPL |
| **Output Parsers** | Parse output LLM | StrOutputParser, JsonOutputParser |

## 1.3. C√†i ƒë·∫∑t

```bash
# Core packages
pip install langchain langchain-core langchain-community

# Ollama integration (d√πng LLM local ‚Äî ph√π h·ª£p TalkingWithData)
pip install langchain-ollama

# SQL & Database tools
pip install langchain-experimental

# Vector store - Qdrant
pip install langchain-qdrant
```

---

# Ph·∫ßn 2: LangChain ‚Äî C√°ch s·ª≠ d·ª•ng Chi ti·∫øt

## 2.1. K·∫øt n·ªëi LLM (Ollama)

### C∆° b·∫£n ‚Äî Chat Model

```python
from langchain_ollama import ChatOllama

# K·∫øt n·ªëi Ollama local (gi·ªëng TalkingWithData)
llm = ChatOllama(
    model="llama3.2",
    base_url="http://localhost:11434",  # ho·∫∑c http://ollama:11434 trong Docker
    temperature=0,  # 0 = deterministic (t·ªët cho SQL generation)
)

# G·ªçi ƒë∆°n gi·∫£n
response = llm.invoke("Xin ch√†o, b·∫°n l√† ai?")
print(response.content)
```

### Embedding Model

```python
from langchain_ollama import OllamaEmbeddings

# D√πng nomic-embed-text (ƒë√£ c√≥ trong TalkingWithData)
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# T·∫°o vector cho 1 ƒëo·∫°n text
vector = embeddings.embed_query("Danh s√°ch ƒë∆°n h√†ng th√°ng 1")
print(f"Vector dimension: {len(vector)}")  # 768

# T·∫°o vectors cho nhi·ªÅu texts
vectors = embeddings.embed_documents([
    "Table orders: id, customer_id, total, created_at",
    "Table customers: id, name, email, phone"
])
```

## 2.2. Prompt Templates

### ChatPromptTemplate

```python
from langchain_core.prompts import ChatPromptTemplate

# T·∫°o prompt template cho text-to-SQL
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a SQL expert. Convert questions to PostgreSQL queries.
Only output the SQL query, nothing else.
Use this schema:
{schema}"""),
    ("human", "{question}")
])

# Format prompt
formatted = prompt.format_messages(
    schema="Table: orders (id INT, customer VARCHAR, total DECIMAL, created_at DATE)",
    question="T·ªïng doanh thu th√°ng 1?"
)

# G·ªçi LLM
response = llm.invoke(formatted)
print(response.content)
# ‚Üí SELECT SUM(total) FROM orders WHERE EXTRACT(MONTH FROM created_at) = 1
```

### FewShotPromptTemplate (d·∫°y LLM b·∫±ng v√≠ d·ª•)

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

# C√°c v√≠ d·ª• m·∫´u (r·∫•t quan tr·ªçng cho text-to-SQL)
examples = [
    {
        "input": "C√≥ bao nhi√™u kh√°ch h√†ng?",
        "output": "SELECT COUNT(*) FROM customers;"
    },
    {
        "input": "Top 5 s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t?",
        "output": "SELECT product_name, SUM(quantity) as total_sold FROM order_items GROUP BY product_name ORDER BY total_sold DESC LIMIT 5;"
    },
    {
        "input": "Doanh thu trung b√¨nh m·ªói th√°ng?",
        "output": "SELECT EXTRACT(MONTH FROM created_at) as month, AVG(total) as avg_revenue FROM orders GROUP BY month ORDER BY month;"
    }
]

# Template cho m·ªói v√≠ d·ª•
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}")
])

# FewShot template
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples
)

# K·∫øt h·ª£p v√†o prompt ch√≠nh
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a SQL expert. Schema:\n{schema}"),
    few_shot_prompt,
    ("human", "{question}")
])
```

## 2.3. Chains (LCEL ‚Äî LangChain Expression Language)

LCEL l√† c√°ch hi·ªán ƒë·∫°i ƒë·ªÉ t·∫°o chain trong LangChain, s·ª≠ d·ª•ng to√°n t·ª≠ `|` (pipe).

### Chain c∆° b·∫£n

```python
from langchain_core.output_parsers import StrOutputParser

# Chain: prompt ‚Üí LLM ‚Üí parse output
chain = prompt | llm | StrOutputParser()

# Ch·∫°y chain
result = chain.invoke({
    "schema": "Table: orders (id INT, total DECIMAL, created_at DATE)",
    "question": "T·ªïng doanh thu?"
})
print(result)  # "SELECT SUM(total) FROM orders;"
```

### Chain ph·ª©c t·∫°p (multi-step)

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# Step 1: Sinh SQL
sql_chain = sql_prompt | llm | StrOutputParser()

# Step 2: Th·ª±c thi SQL (custom function)
def execute_sql(sql_query: str) -> str:
    """Th·ª±c thi SQL tr√™n database ngu·ªìn"""
    from sqlalchemy import create_engine, text
    engine = create_engine("postgresql://user:pass@localhost:5432/mydb")
    with engine.connect() as conn:
        result = conn.execute(text(sql_query))
        rows = result.fetchall()
        return str(rows)

# Step 3: T·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n
answer_prompt = ChatPromptTemplate.from_messages([
    ("system", "D·ª±a tr√™n k·∫øt qu·∫£ SQL, tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n."),
    ("human", "C√¢u h·ªèi: {question}\nSQL: {sql}\nK·∫øt qu·∫£: {result}\n\nTr·∫£ l·ªùi:")
])

# Full chain
full_chain = (
    RunnablePassthrough.assign(
        sql=sql_chain  # b∆∞·ªõc 1: sinh SQL
    )
    | RunnablePassthrough.assign(
        result=lambda x: execute_sql(x["sql"])  # b∆∞·ªõc 2: ch·∫°y SQL
    )
    | answer_prompt  # b∆∞·ªõc 3: format prompt
    | llm           # b∆∞·ªõc 3: g·ªçi LLM
    | StrOutputParser()  # b∆∞·ªõc 3: parse output
)

# Ch·∫°y
answer = full_chain.invoke({
    "schema": "Table: orders (id, total, created_at)",
    "question": "T·ªïng doanh thu th√°ng 1?"
})
```

## 2.4. Memory (B·ªô nh·ªõ h·ªôi tho·∫°i)

```python
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import MessagesPlaceholder

# Memory gi·ªØ l·∫°i N l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t
memory = ConversationBufferWindowMemory(
    k=10,  # gi·ªØ 10 l∆∞·ª£t cu·ªëi
    return_messages=True,
    memory_key="chat_history"
)

# Prompt c√≥ ch·ªó cho history
prompt_with_memory = ChatPromptTemplate.from_messages([
    ("system", "You are a SQL expert. Schema:\n{schema}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{question}")
])

# L∆∞u tin nh·∫Øn
memory.save_context(
    {"input": "C√≥ bao nhi√™u ƒë∆°n h√†ng?"},
    {"output": "SELECT COUNT(*) FROM orders;"}
)

# L·∫•y history
history = memory.load_memory_variables({})["chat_history"]
```

## 2.5. Vector Store ‚Äî Qdrant

```python
from langchain_qdrant import QdrantVectorStore
from langchain_ollama import OllamaEmbeddings
from qdrant_client import QdrantClient

# K·∫øt n·ªëi Qdrant (ƒë√£ c√≥ trong TalkingWithData)
qdrant_client = QdrantClient(host="localhost", port=6333)

embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# T·∫°o vector store
vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name="database_schemas",
    embedding=embeddings
)

# Th√™m documents
from langchain_core.documents import Document

docs = [
    Document(
        page_content="Table: orders (id INT PK, customer_id INT FK, total DECIMAL, created_at TIMESTAMP)",
        metadata={"database": "ecommerce", "table": "orders"}
    ),
    Document(
        page_content="Table: customers (id INT PK, name VARCHAR, email VARCHAR, phone VARCHAR)",
        metadata={"database": "ecommerce", "table": "customers"}
    ),
]

vector_store.add_documents(docs)

# T√¨m ki·∫øm (semantic search)
results = vector_store.similarity_search(
    query="ƒë∆°n h√†ng c·ªßa kh√°ch h√†ng",
    k=3
)
for doc in results:
    print(doc.page_content)
    print(doc.metadata)
```

### Retriever (d√πng trong chain)

```python
# T·∫°o retriever t·ª´ vector store
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# D√πng trong chain
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n".join(doc.page_content for doc in docs)

rag_chain = (
    {
        "schema": retriever | format_docs,  # t·ª± ƒë·ªông t√¨m schema ph√π h·ª£p
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# Ch·∫°y ‚Äî t·ª± ƒë·ªông t√¨m schema li√™n quan r·ªìi sinh SQL
sql = rag_chain.invoke("T·ªïng doanh thu theo t·ª´ng kh√°ch h√†ng?")
```

## 2.6. SQL Database Tools & Agent

LangChain c√≥ s·∫µn tools cho SQL:

```python
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain.agents import create_sql_agent

# K·∫øt n·ªëi database
db = SQLDatabase.from_uri("postgresql://user:pass@localhost:5432/mydb")

# Xem th√¥ng tin
print(db.get_usable_table_names())
print(db.get_table_info())  # Schema ƒë·∫ßy ƒë·ªß

# T·∫°o toolkit
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

# T·∫°o SQL Agent (t·ª± ƒë·ªông sinh SQL, ch·∫°y, s·ª≠a l·ªói)
agent = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    agent_type="tool-calling",
    verbose=True  # In ra t·ª´ng b∆∞·ªõc suy lu·∫≠n
)

# H·ªèi
result = agent.invoke({
    "input": "Top 3 kh√°ch h√†ng c√≥ doanh thu cao nh·∫•t?"
})
print(result["output"])
```

**SQL Agent t·ª± ƒë·ªông:**
1. Xem schema database
2. Sinh c√¢u SQL
3. Th·ª±c thi SQL
4. N·∫øu l·ªói ‚Üí t·ª± s·ª≠a SQL v√† ch·∫°y l·∫°i
5. Format k·∫øt qu·∫£ th√†nh c√¢u tr·∫£ l·ªùi

## 2.7. Custom Tools

```python
from langchain_core.tools import tool

@tool
def search_schema(query: str) -> str:
    """T√¨m ki·∫øm schema database ph√π h·ª£p v·ªõi c√¢u h·ªèi."""
    results = vector_store.similarity_search(query, k=5)
    return "\n".join(doc.page_content for doc in results)

@tool
def execute_sql_query(sql_query: str) -> str:
    """Th·ª±c thi c√¢u SQL tr√™n database v√† tr·∫£ v·ªÅ k·∫øt qu·∫£."""
    from sqlalchemy import create_engine, text
    engine = create_engine("postgresql://user:pass@localhost:5432/mydb")
    try:
        with engine.connect() as conn:
            result = conn.execute(text(sql_query))
            rows = result.fetchall()
            columns = list(result.keys())
            return str([dict(zip(columns, row)) for row in rows[:50]])
    except Exception as e:
        return f"Error: {str(e)}"

# Agent v·ªõi custom tools
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a data analyst assistant. 
    When asked a question:
    1. First search for relevant database schema
    2. Generate a SQL query based on the schema
    3. Execute the SQL query
    4. Provide a clear answer"""),
    MessagesPlaceholder(variable_name="chat_history", optional=True),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent = create_tool_calling_agent(
    llm=llm,
    tools=[search_schema, execute_sql_query],
    prompt=agent_prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=[search_schema, execute_sql_query],
    verbose=True,
    max_iterations=5
)

# Ch·∫°y
result = agent_executor.invoke({"input": "T·ªïng doanh thu th√°ng n√†y l√† bao nhi√™u?"})
```

## 2.8. Output Parsers

```python
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

# ƒê·ªãnh nghƒ©a output format
class SQLResult(BaseModel):
    sql_query: str = Field(description="The SQL query")
    explanation: str = Field(description="Brief explanation of the query")

parser = JsonOutputParser(pydantic_object=SQLResult)

prompt_with_format = ChatPromptTemplate.from_messages([
    ("system", "You are a SQL expert. {format_instructions}"),
    ("human", "Schema: {schema}\nQuestion: {question}")
]).partial(format_instructions=parser.get_format_instructions())

chain = prompt_with_format | llm | parser

result = chain.invoke({
    "schema": "Table: orders (id, total, created_at)",
    "question": "T·ªïng doanh thu?"
})
# result = {"sql_query": "SELECT SUM(total) FROM orders;", "explanation": "..."}
```

---

# Ph·∫ßn 3: Vanna 2.0 ‚Äî Text-to-SQL chuy√™n bi·ªát

## 3.1. Vanna l√† g√¨?

**Vanna** l√† th∆∞ vi·ªán Python **chuy√™n bi·ªát cho Text-to-SQL**, s·ª≠ d·ª•ng k·ªπ thu·∫≠t **RAG (Retrieval-Augmented Generation)**. Kh√¥ng gi·ªëng LangChain (general-purpose), Vanna t·∫≠p trung ho√†n to√†n v√†o vi·ªác chuy·ªÉn ng√¥n ng·ªØ t·ª± nhi√™n th√†nh SQL.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Vanna 2.0                         ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ   ‚îÇ  Training ‚îÇ     ‚îÇ    RAG    ‚îÇ     ‚îÇ   SQL    ‚îÇ ‚îÇ
‚îÇ   ‚îÇ  Data     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Retrieval‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇGeneration‚îÇ ‚îÇ
‚îÇ   ‚îÇ  (DDL,    ‚îÇ     ‚îÇ  (Vector  ‚îÇ     ‚îÇ  (LLM)   ‚îÇ ‚îÇ
‚îÇ   ‚îÇ   Q&A,    ‚îÇ     ‚îÇ   Search) ‚îÇ     ‚îÇ          ‚îÇ ‚îÇ
‚îÇ   ‚îÇ   docs)   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ   Supported LLMs: Ollama, OpenAI, Anthropic, ...    ‚îÇ
‚îÇ   Supported VectorDBs: Qdrant, ChromaDB, ...        ‚îÇ
‚îÇ   Supported DBs: PostgreSQL, MySQL, SQLite, ...     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Vanna 2.0 vs 1.x

| T√≠nh nƒÉng | Vanna 1.x | Vanna 2.0 |
|-----------|-----------|-----------|
| Ki·∫øn tr√∫c | Monolithic | Modular (plugin-based) |
| LLM | Ch·ªâ OpenAI/Mistral | B·∫•t k·ª≥ (Ollama, OpenAI, ...) |
| Vector Store | ChromaDB built-in | B·∫•t k·ª≥ (Qdrant, ChromaDB, ...) |
| Customization | H·∫°n ch·∫ø | T·ª± do k·∫øt h·ª£p components |
| Training | C∆° b·∫£n | DDL + Documentation + Q&A pairs |

## 3.2. C√†i ƒë·∫∑t

```bash
# Core
pip install vanna

# V·ªõi Ollama (local LLM)
pip install 'vanna[ollama]'

# V·ªõi Qdrant (vector store)
pip install 'vanna[qdrant]'

# Ho·∫∑c c√†i h·∫øt
pip install 'vanna[ollama,qdrant]'
```

## 3.3. Ki·∫øn tr√∫c Vanna 2.0

Vanna 2.0 d√πng **mixin pattern** ‚Äî b·∫°n t·∫°o class k·∫øt h·ª£p LLM + VectorStore t√πy √Ω:

```python
# K·∫øt h·ª£p: Ollama (LLM) + Qdrant (Vector Store)
from vanna.ollama import Ollama
from vanna.qdrant import Qdrant_VectorStore

class MyVanna(Qdrant_VectorStore, Ollama):
    def __init__(self, config=None):
        Qdrant_VectorStore.__init__(self, config=config)
        Ollama.__init__(self, config=config)
```

C√°c k·∫øt h·ª£p c√≥ th·ªÉ:

| LLM (ch·ªçn 1) | Vector Store (ch·ªçn 1) |
|---------------|----------------------|
| `Ollama` | `Qdrant_VectorStore` |
| `OpenAI_Chat` | `ChromaDB_VectorStore` |
| `Anthropic_Chat` | `Pinecone_VectorStore` |
| `Mistral` | `FAISS_VectorStore` |
| Custom class | Custom class |

## 3.4. Setup Vanna v·ªõi Ollama + Qdrant

```python
from vanna.ollama import Ollama
from vanna.qdrant import Qdrant_VectorStore


class TalkWithDataVanna(Qdrant_VectorStore, Ollama):
    def __init__(self, config=None):
        Qdrant_VectorStore.__init__(self, config=config)
        Ollama.__init__(self, config=config)


# Kh·ªüi t·∫°o
vn = TalkWithDataVanna(config={
    # Ollama config
    "model": "llama3.2",
    "ollama_host": "http://localhost:11434",
    
    # Qdrant config  
    "qdrant_host": "localhost",
    "qdrant_port": 6333,
    "collection_name": "talkwdata_schemas",
    
    # Embedding config (Vanna t·ª± d√πng Ollama ƒë·ªÉ t·∫°o embedding)
    "embedding_model": "nomic-embed-text"
})

# K·∫øt n·ªëi database ngu·ªìn
vn.connect_to_postgres(
    host="localhost",
    port=5432,
    dbname="sample_db",
    user="user",
    password="password"
)
```

## 3.5. Training ‚Äî D·∫°y Vanna hi·ªÉu Database

### Training b·∫±ng DDL (c·∫•u tr√∫c b·∫£ng)

```python
# C√°ch 1: Train t·ª´ DDL string
vn.train(ddl="""
    CREATE TABLE orders (
        id SERIAL PRIMARY KEY,
        customer_id INTEGER REFERENCES customers(id),
        total DECIMAL(10,2) NOT NULL,
        status VARCHAR(20) DEFAULT 'pending',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    
    CREATE TABLE customers (
        id SERIAL PRIMARY KEY,
        name VARCHAR(100) NOT NULL,
        email VARCHAR(255) UNIQUE,
        phone VARCHAR(20),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    
    CREATE TABLE order_items (
        id SERIAL PRIMARY KEY,
        order_id INTEGER REFERENCES orders(id),
        product_id INTEGER REFERENCES products(id),
        quantity INTEGER NOT NULL,
        unit_price DECIMAL(10,2) NOT NULL
    );
    
    CREATE TABLE products (
        id SERIAL PRIMARY KEY,
        name VARCHAR(100) NOT NULL,
        category VARCHAR(50),
        price DECIMAL(10,2) NOT NULL,
        stock INTEGER DEFAULT 0
    );
""")

# C√°ch 2: Auto-train t·ª´ database (ƒë·ªçc information_schema)
# Vanna t·ª± k·∫øt n·ªëi DB v√† extract DDL
df_information_schema = vn.run_sql("SELECT * FROM INFORMATION_SCHEMA.COLUMNS")
plan = vn.get_training_plan_generic(df_information_schema)
vn.train(plan=plan)
```

### Training b·∫±ng Documentation (m√¥ t·∫£ nghi·ªáp v·ª•)

```python
# Gi√∫p LLM hi·ªÉu ng·ªØ c·∫£nh nghi·ªáp v·ª•
vn.train(documentation="""
    H·ªá th·ªëng qu·∫£n l√Ω b√°n h√†ng:
    - B·∫£ng orders: l∆∞u ƒë∆°n h√†ng, status c√≥ th·ªÉ l√† 'pending', 'confirmed', 'shipped', 'delivered', 'cancelled'
    - B·∫£ng customers: th√¥ng tin kh√°ch h√†ng  
    - Doanh thu = SUM(total) c·ªßa orders c√≥ status = 'delivered'
    - Kh√°ch h√†ng VIP = kh√°ch c√≥ t·ªïng ƒë∆°n h√†ng > 10 tri·ªáu
    - Th√°ng t√†i ch√≠nh b·∫Øt ƒë·∫ßu t·ª´ ng√†y 1
""")

# M√¥ t·∫£ t·ª´ng b·∫£ng
vn.train(documentation="B·∫£ng orders.status: 'pending'=ch·ªù x√°c nh·∫≠n, 'confirmed'=ƒë√£ x√°c nh·∫≠n, 'shipped'=ƒëang giao, 'delivered'=ƒë√£ giao, 'cancelled'=ƒë√£ h·ªßy")
vn.train(documentation="B·∫£ng products.category: 'electronics', 'clothing', 'food', 'books'")
```

### Training b·∫±ng Question-SQL pairs (v√≠ d·ª• m·∫´u)

```python
# D·∫°y Vanna b·∫±ng c√°c c·∫∑p c√¢u h·ªèi-SQL m·∫´u
vn.train(
    question="C√≥ bao nhi√™u ƒë∆°n h√†ng trong th√°ng n√†y?",
    sql="SELECT COUNT(*) FROM orders WHERE DATE_TRUNC('month', created_at) = DATE_TRUNC('month', CURRENT_DATE);"
)

vn.train(
    question="Top 5 kh√°ch h√†ng c√≥ doanh thu cao nh·∫•t?",
    sql="""
        SELECT c.name, SUM(o.total) as total_revenue
        FROM customers c
        JOIN orders o ON c.id = o.customer_id
        WHERE o.status = 'delivered'
        GROUP BY c.id, c.name
        ORDER BY total_revenue DESC
        LIMIT 5;
    """
)

vn.train(
    question="S·∫£n ph·∫©m n√†o b√°n ch·∫°y nh·∫•t?",
    sql="""
        SELECT p.name, SUM(oi.quantity) as total_sold
        FROM products p
        JOIN order_items oi ON p.id = oi.product_id
        GROUP BY p.id, p.name
        ORDER BY total_sold DESC
        LIMIT 1;
    """
)

vn.train(
    question="Doanh thu trung b√¨nh m·ªói ƒë∆°n h√†ng?",
    sql="SELECT AVG(total) as avg_order_value FROM orders WHERE status = 'delivered';"
)
```

### Xem d·ªØ li·ªáu training

```python
# Xem t·∫•t c·∫£ training data ƒë√£ l∆∞u
training_data = vn.get_training_data()
print(training_data)

# X√≥a 1 training data
vn.remove_training_data(id="xxx")
```

## 3.6. Sinh SQL ‚Äî S·ª≠ d·ª•ng

### C∆° b·∫£n

```python
# Sinh SQL t·ª´ c√¢u h·ªèi
sql = vn.generate_sql("C√≥ bao nhi√™u ƒë∆°n h√†ng trong th√°ng 1?")
print(sql)
# ‚Üí SELECT COUNT(*) FROM orders WHERE EXTRACT(MONTH FROM created_at) = 1;
```

### Sinh SQL + Ch·∫°y + Tr·∫£ k·∫øt qu·∫£

```python
# Ch·∫°y SQL v√† l·∫•y k·∫øt qu·∫£ (DataFrame)
df = vn.run_sql(sql)
print(df)

# Ho·∫∑c 1 b∆∞·ªõc: h·ªèi ‚Üí SQL ‚Üí ch·∫°y ‚Üí k·∫øt qu·∫£
result = vn.ask("T·ªïng doanh thu theo th√°ng?")
# result ch·ª©a: sql, DataFrame, plotly chart (n·∫øu c√≥)
```

### ask() ‚Äî Full pipeline

```python
result = vn.ask(
    question="Top 5 s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t th√°ng n√†y?",
    print_results=True,     # In k·∫øt qu·∫£
    auto_train=True,        # T·ª± ƒë·ªông l∆∞u Q&A pair n·∫øu user confirm
    visualize=True          # T·∫°o chart Plotly
)

# result tr·∫£ v·ªÅ:
# - result.sql: c√¢u SQL
# - result.df: DataFrame k·∫øt qu·∫£
# - result.fig: Plotly figure (n·∫øu ph√π h·ª£p)
# - result.summary: T√≥m t·∫Øt k·∫øt qu·∫£
```

## 3.7. Follow-up Questions

```python
# Vanna g·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
followups = vn.generate_followup_questions(
    question="T·ªïng doanh thu th√°ng n√†y?",
    sql=sql,
    df=df
)
print(followups)
# ‚Üí ["Doanh thu so v·ªõi th√°ng tr∆∞·ªõc th·∫ø n√†o?",
#     "Kh√°ch h√†ng n√†o ƒë√≥ng g√≥p nhi·ªÅu nh·∫•t?",
#     "Xu h∆∞·ªõng doanh thu 6 th√°ng g·∫ßn ƒë√¢y?"]
```

## 3.8. Vanna Flask UI (bonus)

```python
from vanna.flask import VannaFlaskApp

# T·∫°o web UI ƒë∆°n gi·∫£n
app = VannaFlaskApp(vn)
app.run()
# ‚Üí M·ªü browser http://localhost:8084
```

---

# Ph·∫ßn 4: So s√°nh LangChain vs Vanna 2.0

## 4.1. B·∫£ng so s√°nh

| Ti√™u ch√≠ | LangChain | Vanna 2.0 |
|----------|-----------|-----------|
| **M·ª•c ƒë√≠ch** | General-purpose LLM framework | Chuy√™n bi·ªát Text-to-SQL |
| **ƒê·ªô ph·ª©c t·∫°p** | Cao (nhi·ªÅu concept) | Th·∫•p (focus v√†o SQL) |
| **Learning curve** | D·ªëc | D·ªÖ ti·∫øp c·∫≠n |
| **Customization** | R·∫•t linh ho·∫°t | V·ª´a ph·∫£i |
| **Text-to-SQL** | C·∫ßn t·ª± build chain/agent | Built-in, out-of-the-box |
| **Training (RAG)** | T·ª± implement | `vn.train()` ‚Äî 1 d√≤ng |
| **Auto-correction** | T·ª± implement | T·ª± ƒë·ªông retry khi SQL l·ªói |
| **Visualization** | Kh√¥ng c√≥ | Plotly charts t·ª± ƒë·ªông |
| **Memory** | C√≥ (nhi·ªÅu lo·∫°i) | H·∫°n ch·∫ø |
| **Agent** | M·∫°nh (multi-tool) | Kh√¥ng c√≥ |
| **Ecosystem** | R·∫•t l·ªõn | Nh·ªè, chuy√™n bi·ªát |
| **Ollama support** | ‚úÖ T·ªët | ‚úÖ T·ªët |
| **Qdrant support** | ‚úÖ T·ªët | ‚úÖ T·ªët |

## 4.2. Khi n√†o d√πng c√°i n√†o?

### D√πng **Vanna 2.0** khi:
- Focus ch√≠nh l√† Text-to-SQL
- Mu·ªën setup nhanh, √≠t code
- C·∫ßn training data management built-in
- C·∫ßn auto-correction SQL
- Demo/prototype nhanh

### D√πng **LangChain** khi:
- C·∫ßn control chi ti·∫øt t·ª´ng b∆∞·ªõc
- C·∫ßn memory/conversation management ph·ª©c t·∫°p
- C·∫ßn k·∫øt h·ª£p nhi·ªÅu tools (kh√¥ng ch·ªâ SQL)
- C·∫ßn custom agent behavior
- D·ª± √°n ph·ª©c t·∫°p, nhi·ªÅu integration

### K·∫øt h·ª£p c·∫£ hai:
- D√πng **Vanna** cho core Text-to-SQL engine
- D√πng **LangChain** cho conversation management, memory, v√† c√°c t√≠nh nƒÉng ph·ª•

---

# Ph·∫ßn 5: √Åp d·ª•ng v√†o TalkingWithData

## 5.1. Ph∆∞∆°ng √°n ƒë·ªÅ xu·∫•t

D·ª±a tr√™n ki·∫øn tr√∫c hi·ªán t·∫°i c·ªßa TalkingWithData (Ollama + Qdrant + PostgreSQL + FastAPI), c√≥ 3 ph∆∞∆°ng √°n:

### Ph∆∞∆°ng √°n A: D√πng Vanna 2.0 (üèÜ ƒê·ªÅ xu·∫•t cho Demo)

```
∆Øu ƒëi·ªÉm: Setup nhanh, √≠t code, t·ª± ƒë·ªông training t·ª´ DB, auto-correction
Nh∆∞·ª£c ƒëi·ªÉm: √çt ki·ªÉm so√°t chi ti·∫øt

User Question ‚Üí Vanna (RAG + LLM) ‚Üí SQL ‚Üí Execute ‚Üí Answer
```

### Ph∆∞∆°ng √°n B: D√πng LangChain

```
∆Øu ƒëi·ªÉm: Linh ho·∫°t, ki·ªÉm so√°t m·ªçi b∆∞·ªõc, conversation memory
Nh∆∞·ª£c ƒëi·ªÉm: Code nhi·ªÅu h∆°n, ph·∫£i t·ª± x·ª≠ l√Ω error recovery

User Question ‚Üí Retriever (Qdrant) ‚Üí Prompt + Schema ‚Üí LLM ‚Üí SQL ‚Üí Execute ‚Üí LLM ‚Üí Answer
```

### Ph∆∞∆°ng √°n C: K·∫øt h·ª£p Vanna + LangChain

```
∆Øu ƒëi·ªÉm: Best of both worlds
Nh∆∞·ª£c ƒëi·ªÉm: Complexity cao h∆°n

Vanna: Text-to-SQL core
LangChain: Conversation memory + Additional tools
```

## 5.2. Tri·ªÉn khai Ph∆∞∆°ng √°n A ‚Äî Vanna 2.0

### B∆∞·ªõc 1: C·∫≠p nh·∫≠t `requirements.txt`

Th√™m:
```
vanna[ollama,qdrant]==0.7.5
```

### B∆∞·ªõc 2: T·∫°o Vanna instance ‚Äî `shared/vanna_client.py`

```python
"""
Vanna 2.0 client cho TalkingWithData
K·∫øt h·ª£p: Ollama (LLM local) + Qdrant (Vector Store)
"""
from vanna.ollama import Ollama
from vanna.qdrant import Qdrant_VectorStore
import os
from dotenv import load_dotenv
from pathlib import Path

env_path = Path(__file__).parent.parent / ".server.env"
load_dotenv(dotenv_path=env_path)

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_DEFAULT_MODEL = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.2")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))


class TalkWithDataVanna(Qdrant_VectorStore, Ollama):
    """Custom Vanna class cho TalkingWithData"""
    def __init__(self, config=None):
        Qdrant_VectorStore.__init__(self, config=config)
        Ollama.__init__(self, config=config)


# Singleton instance
_vanna_instance = None


def get_vanna() -> TalkWithDataVanna:
    """L·∫•y Vanna instance (singleton pattern)"""
    global _vanna_instance
    
    if _vanna_instance is None:
        _vanna_instance = TalkWithDataVanna(config={
            # Ollama
            "model": OLLAMA_DEFAULT_MODEL,
            "ollama_host": OLLAMA_BASE_URL,
            
            # Qdrant
            "qdrant_host": QDRANT_HOST,
            "qdrant_port": QDRANT_PORT,
            "collection_name": "talkwdata_vanna",
            
            # Embedding (Vanna d√πng Ollama embed)
            "embedding_model": "nomic-embed-text"
        })
    
    return _vanna_instance


def connect_database(connection_string: str):
    """K·∫øt n·ªëi Vanna ƒë·∫øn database ngu·ªìn"""
    vn = get_vanna()
    # Parse connection string
    # postgresql://user:pass@host:port/dbname
    from urllib.parse import urlparse
    parsed = urlparse(connection_string)
    
    vn.connect_to_postgres(
        host=parsed.hostname,
        port=parsed.port or 5432,
        dbname=parsed.path.lstrip('/'),
        user=parsed.username,
        password=parsed.password
    )


def train_from_database(connection_string: str):
    """Auto-train Vanna t·ª´ database schema"""
    vn = get_vanna()
    connect_database(connection_string)
    
    # L·∫•y th√¥ng tin schema
    df = vn.run_sql("SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = 'public'")
    
    # T·∫°o training plan
    plan = vn.get_training_plan_generic(df)
    
    # Train
    vn.train(plan=plan)
    
    return {"message": "Training completed", "tables": len(df['table_name'].unique())}


def train_ddl(ddl: str):
    """Train Vanna b·∫±ng DDL string"""
    vn = get_vanna()
    vn.train(ddl=ddl)


def train_documentation(doc: str):
    """Train Vanna b·∫±ng documentation"""
    vn = get_vanna()
    vn.train(documentation=doc)


def train_question_sql(question: str, sql: str):
    """Train Vanna b·∫±ng c·∫∑p question-SQL"""
    vn = get_vanna()
    vn.train(question=question, sql=sql)


def ask_question(question: str) -> dict:
    """
    H·ªèi d·ªØ li·ªáu b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n
    
    Returns:
        dict: {
            "question": str,
            "sql_query": str,
            "answer": str,
            "query_result": list[dict] | None,
            "followup_questions": list[str]
        }
    """
    vn = get_vanna()
    
    # 1. Sinh SQL
    sql = vn.generate_sql(question)
    
    if not sql or "CANNOT" in sql.upper():
        return {
            "question": question,
            "sql_query": None,
            "answer": "Kh√¥ng th·ªÉ t·∫°o c√¢u truy v·∫•n t·ª´ c√¢u h·ªèi n√†y.",
            "query_result": None,
            "followup_questions": []
        }
    
    # 2. Th·ª±c thi SQL
    try:
        df = vn.run_sql(sql)
        query_result = df.to_dict(orient='records') if df is not None else None
    except Exception as e:
        return {
            "question": question,
            "sql_query": sql,
            "answer": f"L·ªói khi th·ª±c thi SQL: {str(e)}",
            "query_result": None,
            "followup_questions": []
        }
    
    # 3. T·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n
    try:
        summary = vn.generate_summary(question=question, df=df)
    except Exception:
        summary = f"K·∫øt qu·∫£ truy v·∫•n: {query_result}"
    
    # 4. G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
    try:
        followups = vn.generate_followup_questions(
            question=question, sql=sql, df=df
        )
    except Exception:
        followups = []
    
    return {
        "question": question,
        "sql_query": sql,
        "answer": summary,
        "query_result": query_result,
        "followup_questions": followups or []
    }


def get_training_data():
    """Xem d·ªØ li·ªáu training ƒë√£ l∆∞u"""
    vn = get_vanna()
    return vn.get_training_data()


def remove_training_data(training_id: str):
    """X√≥a 1 training data"""
    vn = get_vanna()
    vn.remove_training_data(id=training_id)
```

### B∆∞·ªõc 3: C·∫≠p nh·∫≠t text_to_data service d√πng Vanna

Thay th·∫ø `text_to_data_service.py` ph·∫ßn `process_question()`:

```python
# Trong module/text_to_data/service/text_to_data_service.py

from shared.vanna_client import (
    ask_question, train_from_database, train_ddl, 
    train_documentation, train_question_sql, connect_database
)


def process_question(db: Session, question: str, database_name: str = None) -> dict:
    """
    Core: H·ªèi d·ªØ li·ªáu qua Vanna 2.0
    
    Vanna t·ª± ƒë·ªông:
    1. T√¨m schema ph√π h·ª£p (RAG t·ª´ Qdrant)
    2. Sinh SQL (LLM Ollama)
    3. Th·ª±c thi SQL
    4. T·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n
    """
    # N·∫øu c√≥ database_name, k·∫øt n·ªëi ƒë·∫øn DB ƒë√≥ tr∆∞·ªõc
    if database_name:
        connection = db.query(DatabaseConnection).filter(
            DatabaseConnection.name == database_name,
            DatabaseConnection.is_active == True
        ).first()
        if connection:
            connect_database(connection.connection_string)
    
    result = ask_question(question)
    
    return {
        "question": result["question"],
        "sql_query": result.get("sql_query"),
        "answer": result.get("answer", "Kh√¥ng th·ªÉ tr·∫£ l·ªùi."),
        "query_result": result.get("query_result"),
        "schema_context": None
    }


def import_and_train(db: Session, data: SchemaImportRequest) -> dict:
    """Import schema + Auto-train Vanna"""
    connection = db.query(DatabaseConnection).filter(
        DatabaseConnection.id == data.connection_id
    ).first()
    
    if not connection:
        raise HTTPException(status_code=404, detail="Connection not found")
    
    # Vanna auto-train t·ª´ database
    result = train_from_database(connection.connection_string)
    
    return result
```

### B∆∞·ªõc 4: Th√™m Training endpoints

```python
# module/text_to_data/endpoint/training_endpoint.py

from fastapi import APIRouter, Depends
from pydantic import BaseModel
from typing import Optional
from core.dependencies import get_current_user_oauth2
from module.auth.model.user import User
from shared.vanna_client import (
    train_ddl, train_documentation, train_question_sql,
    get_training_data, remove_training_data
)

router = APIRouter()


class TrainDDLRequest(BaseModel):
    ddl: str

class TrainDocRequest(BaseModel):
    documentation: str

class TrainQARequest(BaseModel):
    question: str
    sql: str


@router.post("/train/ddl")
def train_with_ddl(
    data: TrainDDLRequest,
    current_user: User = Depends(get_current_user_oauth2)
):
    """Train Vanna b·∫±ng DDL (c·∫•u tr√∫c b·∫£ng)"""
    train_ddl(data.ddl)
    return {"message": "DDL training completed"}


@router.post("/train/documentation")
def train_with_doc(
    data: TrainDocRequest,
    current_user: User = Depends(get_current_user_oauth2)
):
    """Train Vanna b·∫±ng documentation (m√¥ t·∫£ nghi·ªáp v·ª•)"""
    train_documentation(data.documentation)
    return {"message": "Documentation training completed"}


@router.post("/train/question-sql")
def train_with_qa(
    data: TrainQARequest,
    current_user: User = Depends(get_current_user_oauth2)
):
    """Train Vanna b·∫±ng c·∫∑p c√¢u h·ªèi - SQL m·∫´u"""
    train_question_sql(data.question, data.sql)
    return {"message": "Q&A training completed"}


@router.get("/train/data")
def list_training_data(
    current_user: User = Depends(get_current_user_oauth2)
):
    """Xem danh s√°ch training data"""
    data = get_training_data()
    return {"training_data": data.to_dict(orient='records') if data is not None else []}


@router.delete("/train/data/{training_id}")
def delete_training_data(
    training_id: str,
    current_user: User = Depends(get_current_user_oauth2)
):
    """X√≥a 1 training data"""
    remove_training_data(training_id)
    return {"message": "Training data removed"}
```

## 5.3. Tri·ªÉn khai Ph∆∞∆°ng √°n B ‚Äî LangChain

### B∆∞·ªõc 1: C·∫≠p nh·∫≠t `requirements.txt`

```
langchain==0.3.20
langchain-core==0.3.40
langchain-community==0.3.18
langchain-ollama==0.3.2
langchain-qdrant==0.2.2
langchain-experimental==0.3.4
```

### B∆∞·ªõc 2: LangChain-based text-to-SQL ‚Äî `shared/langchain_client.py`

```python
"""
LangChain client cho TalkingWithData
Text-to-SQL pipeline: Question ‚Üí Schema Retrieval ‚Üí SQL Generation ‚Üí Execution ‚Üí Answer
"""
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain.memory import ConversationBufferWindowMemory
from qdrant_client import QdrantClient
from sqlalchemy import create_engine, text
import os
from dotenv import load_dotenv
from pathlib import Path

env_path = Path(__file__).parent.parent / ".server.env"
load_dotenv(dotenv_path=env_path)

# ================================================================
# CONFIG
# ================================================================

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.2")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))

# ================================================================
# COMPONENTS
# ================================================================

# LLM
llm = ChatOllama(
    model=OLLAMA_MODEL,
    base_url=OLLAMA_BASE_URL,
    temperature=0,  # Deterministic cho SQL generation
)

# Embeddings
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url=OLLAMA_BASE_URL
)

# Qdrant
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name="langchain_schemas",
    embedding=embeddings
)

# Retriever
schema_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# ================================================================
# PROMPTS
# ================================================================

SQL_SYSTEM_PROMPT = """You are a PostgreSQL expert. Convert natural language questions to SQL queries.

RULES:
1. ONLY generate SELECT queries
2. Use the provided schema exactly as given
3. Return ONLY the raw SQL query, no markdown, no explanation
4. If you cannot answer, respond with exactly: CANNOT_ANSWER
5. Use PostgreSQL syntax
"""

SQL_GENERATION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", SQL_SYSTEM_PROMPT),
    MessagesPlaceholder(variable_name="chat_history", optional=True),
    ("human", """Database schema:
{schema}

Question: {question}

SQL query:""")
])

ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful data analyst. Answer the question based on the SQL result. Use Vietnamese if the question is in Vietnamese. Be concise."),
    ("human", """Question: {question}
SQL: {sql_query}
Result: {query_result}

Answer:""")
])

# ================================================================
# MEMORY (per conversation)
# ================================================================

# L∆∞u tr·ªØ memory theo conversation_id
_memories = {}


def get_memory(conversation_id: str) -> ConversationBufferWindowMemory:
    """L·∫•y ho·∫∑c t·∫°o memory cho conversation"""
    if conversation_id not in _memories:
        _memories[conversation_id] = ConversationBufferWindowMemory(
            k=10,
            return_messages=True,
            memory_key="chat_history"
        )
    return _memories[conversation_id]


def clear_memory(conversation_id: str):
    """X√≥a memory khi x√≥a conversation"""
    _memories.pop(conversation_id, None)


# ================================================================
# SCHEMA MANAGEMENT
# ================================================================

def index_schema(database_name: str, table_name: str, columns: str, description: str = ""):
    """L∆∞u schema v√†o Qdrant Vector Store"""
    content = f"Table: {table_name} ({columns})"
    if description:
        content += f" -- {description}"
    
    doc = Document(
        page_content=content,
        metadata={
            "database_name": database_name,
            "table_name": table_name,
            "columns": columns,
            "description": description
        }
    )
    vector_store.add_documents([doc])


def index_schemas_batch(schemas: list[dict]):
    """Batch index nhi·ªÅu schemas"""
    docs = []
    for s in schemas:
        content = f"Table: {s['table_name']} ({s['columns']})"
        if s.get('description'):
            content += f" -- {s['description']}"
        docs.append(Document(page_content=content, metadata=s))
    
    vector_store.add_documents(docs)


# ================================================================
# TEXT-TO-SQL PIPELINE
# ================================================================

def _format_schema_docs(docs: list[Document]) -> str:
    """Format retrieved documents th√†nh schema string"""
    return "\n".join(doc.page_content for doc in docs)


def _clean_sql(sql: str) -> str:
    """Clean SQL output t·ª´ LLM"""
    sql = sql.strip()
    # Remove markdown code blocks
    if sql.startswith("```sql"):
        sql = sql[6:]
    if sql.startswith("```"):
        sql = sql[3:]
    if sql.endswith("```"):
        sql = sql[:-3]
    return sql.strip()


def _execute_sql(sql_query: str, connection_string: str) -> list[dict]:
    """Th·ª±c thi SQL v√† tr·∫£ k·∫øt qu·∫£"""
    engine = create_engine(connection_string)
    with engine.connect() as conn:
        result = conn.execute(text(sql_query))
        rows = result.fetchall()
        columns = list(result.keys())
        data = [dict(zip(columns, row)) for row in rows]
        return data[:100]  # Gi·ªõi h·∫°n 100 rows


def process_question(
    question: str,
    connection_string: str,
    conversation_id: str = None
) -> dict:
    """
    Full Text-to-SQL pipeline v·ªõi LangChain
    
    Flow:
    1. Retrieve relevant schema (Qdrant)
    2. Generate SQL (LLM + schema context + chat history)
    3. Execute SQL
    4. Generate natural language answer (LLM)
    5. Save to memory
    """
    # 1. Retrieve schema
    schema_docs = schema_retriever.invoke(question)
    schema_context = _format_schema_docs(schema_docs)
    
    if not schema_context:
        return {
            "question": question,
            "sql_query": None,
            "answer": "Kh√¥ng t√¨m th·∫•y schema ph√π h·ª£p.",
            "query_result": None,
            "schema_context": None
        }
    
    # 2. Get chat history (if conversation exists)
    chat_history = []
    if conversation_id:
        memory = get_memory(conversation_id)
        chat_history = memory.load_memory_variables({}).get("chat_history", [])
    
    # 3. Generate SQL
    sql_chain = SQL_GENERATION_PROMPT | llm | StrOutputParser()
    
    raw_sql = sql_chain.invoke({
        "schema": schema_context,
        "question": question,
        "chat_history": chat_history
    })
    
    sql_query = _clean_sql(raw_sql)
    
    # Check if cannot answer
    if "CANNOT_ANSWER" in sql_query.upper():
        return {
            "question": question,
            "sql_query": None,
            "answer": "Kh√¥ng th·ªÉ t·∫°o truy v·∫•n t·ª´ c√¢u h·ªèi n√†y v·ªõi schema hi·ªán t·∫°i.",
            "query_result": None,
            "schema_context": schema_context
        }
    
    # Validate SELECT only
    if not sql_query.upper().strip().startswith("SELECT"):
        return {
            "question": question,
            "sql_query": sql_query,
            "answer": "Ch·ªâ cho ph√©p truy v·∫•n SELECT.",
            "query_result": None,
            "schema_context": schema_context
        }
    
    # 4. Execute SQL
    try:
        query_result = _execute_sql(sql_query, connection_string)
    except Exception as e:
        # Retry: g·ª≠i l·ªói l·∫°i cho LLM ƒë·ªÉ s·ª≠a SQL
        retry_prompt = ChatPromptTemplate.from_messages([
            ("system", SQL_SYSTEM_PROMPT),
            ("human", """Schema: {schema}
Question: {question}
Previous SQL: {previous_sql}
Error: {error}

Fix the SQL query. Return ONLY the corrected SQL:""")
        ])
        
        retry_chain = retry_prompt | llm | StrOutputParser()
        fixed_sql = retry_chain.invoke({
            "schema": schema_context,
            "question": question,
            "previous_sql": sql_query,
            "error": str(e)
        })
        sql_query = _clean_sql(fixed_sql)
        
        try:
            query_result = _execute_sql(sql_query, connection_string)
        except Exception as e2:
            return {
                "question": question,
                "sql_query": sql_query,
                "answer": f"L·ªói th·ª±c thi SQL (ƒë√£ th·ª≠ s·ª≠a): {str(e2)}",
                "query_result": None,
                "schema_context": schema_context
            }
    
    # 5. Generate answer
    answer_chain = ANSWER_PROMPT | llm | StrOutputParser()
    
    answer = answer_chain.invoke({
        "question": question,
        "sql_query": sql_query,
        "query_result": str(query_result[:20])  # Limit cho LLM
    })
    
    # 6. Save to memory
    if conversation_id:
        memory = get_memory(conversation_id)
        memory.save_context(
            {"input": question},
            {"output": f"SQL: {sql_query}\nAnswer: {answer}"}
        )
    
    return {
        "question": question,
        "sql_query": sql_query,
        "answer": answer,
        "query_result": query_result,
        "schema_context": schema_context
    }
```

**∆Øu ƒëi·ªÉm so v·ªõi vi·∫øt tay (Phase 6 trong server-implementation-guide):**
- **Auto-retry**: Khi SQL l·ªói, t·ª± g·ª≠i error cho LLM s·ª≠a l·∫°i
- **Conversation memory**: Nh·ªõ context h·ªôi tho·∫°i
- **LCEL chains**: Code r√µ r√†ng, d·ªÖ debug

## 5.4. Tri·ªÉn khai Ph∆∞∆°ng √°n C ‚Äî K·∫øt h·ª£p Vanna + LangChain

```python
"""
shared/hybrid_client.py
K·∫øt h·ª£p: Vanna (Text-to-SQL core) + LangChain (Memory + Enhancement)
"""
from shared.vanna_client import get_vanna, ask_question
from langchain.memory import ConversationBufferWindowMemory
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv
from pathlib import Path

env_path = Path(__file__).parent.parent / ".server.env"
load_dotenv(dotenv_path=env_path)

# LangChain LLM (cho ph·∫ßn answer enhancement)
llm = ChatOllama(
    model=os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.2"),
    base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
    temperature=0.3
)

# Memory per conversation
_memories = {}


def get_memory(conversation_id: str) -> ConversationBufferWindowMemory:
    if conversation_id not in _memories:
        _memories[conversation_id] = ConversationBufferWindowMemory(
            k=10, return_messages=True, memory_key="chat_history"
        )
    return _memories[conversation_id]


# Enhanced answer prompt
ENHANCED_ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """You are a helpful data analyst for the TalkingWithData platform.
Answer questions based on SQL results. Use Vietnamese if the user asks in Vietnamese.
Be concise and informative. Format numbers with thousands separators."""),
    MessagesPlaceholder(variable_name="chat_history", optional=True),
    ("human", """Question: {question}
SQL query used: {sql_query}
Query result: {query_result}

Provide a clear answer:""")
])


def process_question_hybrid(
    question: str,
    conversation_id: str = None
) -> dict:
    """
    Hybrid pipeline:
    - Vanna: RAG + SQL Generation + Execution  (core engine)
    - LangChain: Memory + Enhanced Answer      (enhancement)
    """
    # 1. L·∫•y chat history
    chat_history = []
    if conversation_id:
        memory = get_memory(conversation_id)
        chat_history = memory.load_memory_variables({}).get("chat_history", [])
    
    # 2. Enhance question v·ªõi context (n·∫øu c√≥ history)
    enhanced_question = question
    if chat_history:
        # N·∫øu c√¢u h·ªèi c√≥ tham chi·∫øu (v√≠ d·ª•: "c√≤n th√°ng 2 th√¨ sao?")
        # ‚Üí LLM rewrite th√†nh c√¢u h·ªèi ƒë·∫ßy ƒë·ªß
        rewrite_prompt = ChatPromptTemplate.from_messages([
            ("system", "Rewrite the follow-up question to be self-contained, using the conversation history. If it's already clear, return it as-is."),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "Follow-up question: {question}\n\nRewritten question:")
        ])
        rewrite_chain = rewrite_prompt | llm | StrOutputParser()
        enhanced_question = rewrite_chain.invoke({
            "question": question,
            "chat_history": chat_history
        }).strip()
    
    # 3. Vanna: Generate SQL + Execute
    vanna_result = ask_question(enhanced_question)
    
    # 4. LangChain: Enhanced answer
    answer_chain = ENHANCED_ANSWER_PROMPT | llm | StrOutputParser()
    
    enhanced_answer = answer_chain.invoke({
        "question": question,
        "sql_query": vanna_result.get("sql_query", "N/A"),
        "query_result": str(vanna_result.get("query_result", []))[:2000],
        "chat_history": chat_history
    })
    
    # 5. Save to memory
    if conversation_id:
        memory = get_memory(conversation_id)
        memory.save_context(
            {"input": question},
            {"output": enhanced_answer}
        )
    
    return {
        "question": question,
        "sql_query": vanna_result.get("sql_query"),
        "answer": enhanced_answer,
        "query_result": vanna_result.get("query_result"),
        "schema_context": None,
        "followup_questions": vanna_result.get("followup_questions", [])
    }
```

**Lu·ªìng hybrid:**
```
User: "T·ªïng doanh thu th√°ng 1?"
  ‚îÇ
  ‚îú‚îÄ LangChain Memory: (tr·ªëng, l∆∞·ª£t ƒë·∫ßu)
  ‚îú‚îÄ Vanna: RAG ‚Üí SQL ‚Üí Execute ‚Üí Raw result
  ‚îú‚îÄ LangChain: Enhanced answer formatting
  ‚îî‚îÄ L∆∞u v√†o Memory

User: "C√≤n th√°ng 2 th√¨ sao?"
  ‚îÇ
  ‚îú‚îÄ LangChain Memory: c√≥ context "t·ªïng doanh thu th√°ng 1"
  ‚îú‚îÄ LangChain Rewrite: "T·ªïng doanh thu th√°ng 2?" (ƒë·∫ßy ƒë·ªß)
  ‚îú‚îÄ Vanna: RAG ‚Üí SQL ‚Üí Execute
  ‚îú‚îÄ LangChain: Enhanced answer (so s√°nh v·ªõi th√°ng 1)
  ‚îî‚îÄ L∆∞u v√†o Memory
```

## 5.5. API endpoints b·ªï sung cho Training

```
POST /text-to-data/train/auto         ‚Üê Auto-train t·ª´ database (Vanna)
POST /text-to-data/train/ddl          ‚Üê Train b·∫±ng DDL
POST /text-to-data/train/documentation ‚Üê Train b·∫±ng m√¥ t·∫£ nghi·ªáp v·ª•
POST /text-to-data/train/question-sql  ‚Üê Train b·∫±ng c·∫∑p Q&A
GET  /text-to-data/train/data         ‚Üê Xem training data
DELETE /text-to-data/train/data/{id}  ‚Üê X√≥a training data
```

## 5.6. T·ªïng k·∫øt ‚Äî ƒê·ªÅ xu·∫•t cho TalkingWithData

| Ph∆∞∆°ng √°n | ƒê·ªô kh√≥ | Th·ªùi gian | Ch·∫•t l∆∞·ª£ng SQL | Ghi ch√∫ |
|-----------|--------|----------|---------------|---------|
| **A: Vanna** | ‚≠ê D·ªÖ | 1-2 ng√†y | ‚≠ê‚≠ê‚≠ê T·ªët | **ƒê·ªÅ xu·∫•t cho demo** ‚Äî √≠t code, t·ª± training |
| **B: LangChain** | ‚≠ê‚≠ê‚≠ê Kh√≥ | 3-5 ng√†y | ‚≠ê‚≠ê Trung b√¨nh | Linh ho·∫°t, control cao |
| **C: Hybrid** | ‚≠ê‚≠ê V·ª´a | 2-3 ng√†y | ‚≠ê‚≠ê‚≠ê‚≠ê R·∫•t t·ªët | Best of both ‚Äî conversation + SQL |

**ƒê·ªÅ xu·∫•t:** B·∫Øt ƒë·∫ßu v·ªõi **Ph∆∞∆°ng √°n A (Vanna)** cho demo, sau ƒë√≥ n√¢ng c·∫•p l√™n **Ph∆∞∆°ng √°n C (Hybrid)** n·∫øu c·∫ßn conversation context.
